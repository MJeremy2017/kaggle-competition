{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7923c736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T03:04:18.528180Z",
     "iopub.status.busy": "2024-01-09T03:04:18.527367Z",
     "iopub.status.idle": "2024-01-09T03:04:18.539405Z",
     "shell.execute_reply": "2024-01-09T03:04:18.538660Z"
    },
    "papermill": {
     "duration": 0.03059,
     "end_time": "2024-01-09T03:04:18.541387",
     "exception": false,
     "start_time": "2024-01-09T03:04:18.510797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.nme.com/wp-content/uploads/2020/09/Brad_Pitt_Se7en.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url=\"https://www.nme.com/wp-content/uploads/2020/09/Brad_Pitt_Se7en.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b09e5c",
   "metadata": {
    "papermill": {
     "duration": 0.01442,
     "end_time": "2024-01-09T03:04:18.570671",
     "exception": false,
     "start_time": "2024-01-09T03:04:18.556251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comprehensive Analysis and Modeling Notebook\n",
    "\n",
    "Welcome to this comprehensive notebook where we dive deep into the world of data analysis and machine learning. This document is meticulously crafted to guide you through various stages of data processing, modeling, and prediction. Here's what to expect:\n",
    "\n",
    "## What This Notebook Offers:\n",
    "1. **Data Preprocessing**: Initial steps to clean and prepare the data for analysis.\n",
    "2. **Exploratory Data Analysis (EDA)**: Insights and patterns unraveled through visual and statistical methods.\n",
    "3. **Feature Engineering**: Enhancing the dataset with new, informative features.\n",
    "4. **Model Development**: Implementation of various machine learning models, including both traditional and advanced techniques.\n",
    "5. **Evaluation and Optimization**: Assessing model performance and tuning them for better accuracy.\n",
    "6. **Ensemble Techniques**: Leveraging the power of multiple models to improve predictions.\n",
    "7. **Final Predictions and Submission**: Preparing the final predictions for submission, demonstrating the practical application of our analysis.\n",
    "\n",
    "## Intended Audience:\n",
    "This notebook is designed for both beginners and experienced practitioners in the field of data science. Whether you're looking to learn new skills, seeking to understand specific methodologies, or aiming to apply advanced techniques in machine learning, this notebook has something to offer.\n",
    "\n",
    "## Feedback and Collaboration:\n",
    "Your feedback is highly appreciated! If you have any suggestions, questions, or ideas for improvement, please feel free to share. Collaboration is the key to success in the ever-evolving field of data science, and your input is invaluable.\n",
    "\n",
    "---\n",
    "\n",
    "Let's embark on this data science journey together and uncover the stories hidden within the data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97604d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T03:04:18.601431Z",
     "iopub.status.busy": "2024-01-09T03:04:18.601156Z",
     "iopub.status.idle": "2024-01-09T03:04:30.306728Z",
     "shell.execute_reply": "2024-01-09T03:04:30.305874Z"
    },
    "papermill": {
     "duration": 11.723365,
     "end_time": "2024-01-09T03:04:30.309047",
     "exception": false,
     "start_time": "2024-01-09T03:04:18.585682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    SentencePieceBPETokenizer\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00236e3",
   "metadata": {
    "papermill": {
     "duration": 0.014727,
     "end_time": "2024-01-09T03:04:30.339316",
     "exception": false,
     "start_time": "2024-01-09T03:04:30.324589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This cell checks if we are rerunning a Kaggle competition kernel:\n",
    "- If `KAGGLE_IS_COMPETITION_RERUN` is set in the environment, it indicates a rerun, and we execute the corresponding logic\n",
    "- Otherwise, we read the sample submission file using pandas, and then write our submission file. This step is crucial for making submissions in Kaggle competitions.\n",
    "- `sys.exit()` is called to gracefully exit the script when not in a rerun mode. This is important to prevent executing further cells unnecessarily after creating the submission file. It will save you time and even though the notebooks takes hours during submission, it will only use a few seconds of your GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a23e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T03:04:30.371080Z",
     "iopub.status.busy": "2024-01-09T03:04:30.370430Z",
     "iopub.status.idle": "2024-01-09T03:04:30.391724Z",
     "shell.execute_reply": "2024-01-09T03:04:30.390439Z"
    },
    "papermill": {
     "duration": 0.039198,
     "end_time": "2024-01-09T03:04:30.393295",
     "exception": true,
     "start_time": "2024-01-09T03:04:30.354097",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    pass\n",
    "else:\n",
    "    sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "    sub.to_csv('submission.csv', index=False)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b02e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Sentence Transformers\n",
    "\n",
    "This cell is dedicated to initializing the tokenizer and model from the Hugging Face `transformers` library for natural language processing tasks:\n",
    "- We import `AutoTokenizer` and `AutoModel` from the `transformers` package, and `torch` from PyTorch.\n",
    "- The `AutoTokenizer.from_pretrained` method is used to load a pre-trained tokenizer for the MiniLM model (`all-MiniLM-L6-v2`). MiniLM is known for its compact size and efficiency while maintaining strong performance, making it ideal for various NLP tasks.\n",
    "- Similarly, `AutoModel.from_pretrained` loads the corresponding MiniLM model. This model is designed for high performance in a wide range of NLP applications.\n",
    "- We check the type of the loaded model using `type(model)` to confirm it's correctly loaded and then display the model's architecture and configuration by simply calling `model`. This provides insights into the model's structure, such as its layers and parameters, which is useful for understanding its capabilities and potential customizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38ba23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T16:50:33.684109Z",
     "iopub.status.busy": "2023-12-30T16:50:33.683284Z",
     "iopub.status.idle": "2023-12-30T16:50:35.44161Z",
     "shell.execute_reply": "2023-12-30T16:50:35.440642Z",
     "shell.execute_reply.started": "2023-12-30T16:50:33.684076Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2')\n",
    "type(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b51ba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now read several files and put them together. Starting with the famous daigt-v2 dataset: https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T16:50:36.546155Z",
     "iopub.status.busy": "2023-12-30T16:50:36.545778Z",
     "iopub.status.idle": "2023-12-30T16:50:38.666395Z",
     "shell.execute_reply": "2023-12-30T16:50:38.665421Z",
     "shell.execute_reply.started": "2023-12-30T16:50:36.546107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "train_1 = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')[['text', 'label']]\n",
    "train_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daa999",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Adding the 7 prompt dataset: https://www.kaggle.com/datasets/carlmcbrideellis/llm-7-prompt-training-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4e36d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T16:50:44.122293Z",
     "iopub.status.busy": "2023-12-30T16:50:44.121565Z",
     "iopub.status.idle": "2023-12-30T16:50:44.905795Z",
     "shell.execute_reply": "2023-12-30T16:50:44.904865Z",
     "shell.execute_reply.started": "2023-12-30T16:50:44.122262Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/llm-7-prompt-training-dataset/train_essays_RDizzl3_seven_v1.csv\")\n",
    "print(train.shape)\n",
    "train = pd.concat([train, train_1])\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08def893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T16:50:45.805382Z",
     "iopub.status.busy": "2023-12-30T16:50:45.804714Z",
     "iopub.status.idle": "2023-12-30T16:50:45.817735Z",
     "shell.execute_reply": "2023-12-30T16:50:45.816824Z",
     "shell.execute_reply.started": "2023-12-30T16:50:45.805352Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f70e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T16:50:46.746372Z",
     "iopub.status.busy": "2023-12-30T16:50:46.745518Z",
     "iopub.status.idle": "2023-12-30T16:50:46.841445Z",
     "shell.execute_reply": "2023-12-30T16:50:46.840311Z",
     "shell.execute_reply.started": "2023-12-30T16:50:46.74634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop_duplicates(subset=['text'])\n",
    "train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723766fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This cell contains the setup and definition of a function to generate embeddings from text using a pre-trained language model:\n",
    "\n",
    "- First, we set the `device` to either 'cuda' or 'cpu' depending on whether CUDA (GPU support) is available. Using 'cuda' enables faster computation on GPU if it's available.\n",
    "\n",
    "- We then move our pre-trained model to the selected device (GPU or CPU). This step ensures that the model computations are optimized for the available hardware.\n",
    "\n",
    "- The `get_embeddings` function is defined to convert input text into embeddings:\n",
    "  - The text is tokenized using the pre-trained tokenizer. Special tokens are added, and the text is truncated as needed for the model.\n",
    "  - The tokenized text is then passed through the model to get the last hidden state. This step is done within a `torch.inference_mode()` context, which reduces memory usage and increases computation speed by disabling gradient calculations.\n",
    "  - We compute the mean of the last hidden state across all tokens to get a single embedding vector for the input text. This embedding represents the entire input text in a high-dimensional space and captures its semantic features.\n",
    "  - The embedding is moved back to the CPU and converted to a NumPy array before being returned. This is done for compatibility with Python's standard data processing tools and for potential use in non-GPU environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15774ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T16:52:30.903357Z",
     "iopub.status.busy": "2023-12-30T16:52:30.902961Z",
     "iopub.status.idle": "2023-12-30T16:52:34.713932Z",
     "shell.execute_reply": "2023-12-30T16:52:34.713136Z",
     "shell.execute_reply.started": "2023-12-30T16:52:30.903326Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  ## for faster computation\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  ## for faster computation\n",
    "\n",
    "model.to(device)\n",
    "def get_embeddings(texts, batch_size=16):\n",
    "    # Ensure texts is a list\n",
    "    if not isinstance(texts, list):\n",
    "        raise ValueError(\"Input must be a list of texts\")\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(total=len(texts), desc='Processing', unit='text')\n",
    "\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize all texts in the batch\n",
    "        inputs = tokenizer.batch_encode_plus(batch_texts,\n",
    "                                             add_special_tokens=True,\n",
    "                                             truncation=True,\n",
    "                                             padding=True,  # Pad to the longest in the batch\n",
    "                                             return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        # Process in batch\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Calculate embeddings for each text\n",
    "        embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "        all_embeddings.extend(embeddings.cpu().numpy())  # Move the embeddings back to CPU\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(len(batch_texts))\n",
    "\n",
    "    pbar.close()\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddabc8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "In this cell, we use the previously defined `get_embeddings` function to generate embeddings for our training dataset:\n",
    "\n",
    "- We start by converting the 'text' column of the `train` DataFrame into a list. This conversion is necessary because our `get_embeddings` function is designed to take a list of texts as input.\n",
    "- The `get_embeddings` function is then called with this list of texts. It processes each text through the pre-trained language model to generate embeddings.\n",
    "- These embeddings capture the semantic essence of each text in a high-dimensional space, making them suitable for various machine learning tasks, including classification, clustering, and similarity comparison.\n",
    "- The resulting `train_embeddings` variable holds the embeddings for the entire training dataset, which can be used for further machine learning or data analysis tasks.\n",
    "\n",
    "This step is crucial in converting raw text data into a numerical format that machine learning algorithms can efficiently process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20efee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T16:52:40.06204Z",
     "iopub.status.busy": "2023-12-30T16:52:40.061675Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_embeddings = get_embeddings(train['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09df721",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_embeddings = get_embeddings(test['text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10e2eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This cell is focused on computing the cosine similarity between a set of specific topics and the texts in our training and test datasets:\n",
    "\n",
    "- We start by defining a list of seven topics from the PERSUADE dataset. These range from political systems and space exploration to technological advancements and unique human experiences and are written as a general description of what these texts contain.\n",
    "\n",
    "- The `get_embeddings` function, previously defined, is used to generate embeddings for these topics. This transforms the thematic textual descriptions into numerical vectors in a high-dimensional semantic space.\n",
    "\n",
    "- The embeddings for both the training texts (`train_embeddings`) and the topics (`topic_embeddings`) are normalized using `normalize` from `sklearn.preprocessing`. Normalization is probably not necessary for meaningful cosine similarity calculations, but I just want it ensures that the length of each vector is 1, allowing the cosine similarity to effectively measure the angle between the vectors.\n",
    "\n",
    "- `cosine_similarity` from `sklearn.metrics.pairwise` is then used to calculate the cosine similarity between the normalized embeddings of the training texts and the topics. This results in a matrix (`train_cosine_similarities`) where each element represents the similarity between a topic and a training text.\n",
    "\n",
    "- The same process is applied to the test dataset embeddings (`test_embeddings`) to create `test_cosine_similarities`.\n",
    "\n",
    "- We add 1 to the cosine similarity values to shift the range from [-1, 1] to [0, 2]. This adjustment can be useful for subsequent processing steps, especially those that might assume non-negative input.\n",
    "\n",
    "- The resulting `train_cosine_similarities` matrix is displayed for inspection.\n",
    "\n",
    "- Finally, to manage memory usage, embeddings and normalized data are deleted, and garbage collection is invoked.\n",
    "\n",
    "This step is crucial for understanding the relationship between the predefined topics and the corpus of texts, potentially aiding in tasks such as topic modeling, recommendation systems, or thematic text clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a92414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T17:06:40.646472Z",
     "iopub.status.busy": "2023-12-30T17:06:40.64622Z",
     "iopub.status.idle": "2023-12-30T17:06:40.862925Z",
     "shell.execute_reply": "2023-12-30T17:06:40.861385Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.646451Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_embeddings are already calculated for your earlier texts\n",
    "# and train['text'] is your list of earlier texts\n",
    "\n",
    "# Topics\n",
    "topics = [\n",
    "    \"the advantages of limiting car usage\",\n",
    "    \"Arguments in favor of the electoral college system or in favor of changing the electoral college system to a popular vote for the president of the United States\",\n",
    "    \"The challenge of Exploring Venus: It is a worthy pursuit despite of the risks it represents\",\n",
    "    \"Unmasking the face on Mars: Convincing someone that The Face on Mars is a natural landform and not created by aliens\",\n",
    "    \"Making Mona Lisa smile: Facial action coding system as a technology that enables computers to identify human emotions. How valuable it to Open the doors to evaluate human expressions of students in the classroom\",\n",
    "    \"A Cowboy Who Rode the Waves: writing an article from Luke's point of view to convince others to participate in the seagoing cowboys program, which allowed Luke to experience adventures and visit many unique places\",\n",
    "    \"Driverless cars are coming: arguments for and agains the developments of these cars taking into account the advantages and disadvantages of doing so.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the topics\n",
    "topic_embeddings = get_embeddings(topics)\n",
    "\n",
    "# Normalize the embeddings\n",
    "train_embeddings_normalized = normalize(train_embeddings)\n",
    "test_embeddings_normalized = normalize(test_embeddings)\n",
    "topic_embeddings_normalized = normalize(topic_embeddings)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "# This will create a matrix with the similarity of each topic to each text\n",
    "train_cosine_similarities = 1 + cosine_similarity(train_embeddings_normalized, topic_embeddings_normalized)\n",
    "test_cosine_similarities = 1 + cosine_similarity(test_embeddings_normalized, topic_embeddings_normalized)\n",
    "\n",
    "train_cosine_similarities \n",
    "\n",
    "del train_embeddings, test_embeddings, topic_embeddings, train_embeddings_normalized, topic_embeddings_normalized, test_embeddings_normalized\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f04e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This cell defines a function `keep_max_similarity_above_threshold_sparse` and applies it to our cosine similarity matrices. The purpose of this function is twofold:\n",
    "\n",
    "1. **Sparse Matrix Creation**: For each row in the input matrix (representing cosine similarities between texts and topics), the function identifies the maximum similarity value. If this value exceeds a specified threshold (1.4 in this case), the function retains this value; otherwise, it's ignored. This selective process is used to create a sparse matrix (CSR format), which is memory-efficient and retains only the most significant similarity scores.\n",
    "\n",
    "2. **Generating Optimization Weights**: Alongside creating the sparse matrix, the function generates an array (`max_values_per_row`) containing the highest similarity score for each row. If the maximum value in a row doesn't exceed the threshold, a default value of 0.25 is assigned. This array can serve as \"optimization weights\" in subsequent processes, such as weighted machine learning models, where these weights can help emphasize texts with higher thematic relevance based on the defined threshold.\n",
    "\n",
    "- The function is applied to both the training (`train_cosine_similarities`) and test (`test_cosine_similarities`) cosine similarity matrices. As a result, we obtain `train_cosine_similarity_sparse` and `test_cosine_similarity_sparse` for compact representation, and `optimization_weights` as potential input for weighted model training.\n",
    "\n",
    "- This approach is particularly useful for focusing on highly relevant thematic connections and reducing noise from less significant ones, thus optimizing the dataset for more focused and efficient processing in downstream tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c38538",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T17:06:40.8636Z",
     "iopub.status.idle": "2023-12-30T17:06:40.863926Z",
     "shell.execute_reply": "2023-12-30T17:06:40.86378Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.863765Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def keep_max_similarity_above_threshold_sparse(matrix, threshold = 1.4):\n",
    "    # Find the indices of the maximum values in each row\n",
    "    max_indices = np.argmax(matrix, axis=1)\n",
    "\n",
    "    # Extract the maximum values using these indices\n",
    "    max_values = matrix[np.arange(matrix.shape[0]), max_indices]\n",
    "\n",
    "    # Mask to identify rows where the maximum value exceeds the threshold\n",
    "    mask = max_values > threshold\n",
    "\n",
    "    # Create row indices for the sparse matrix\n",
    "    row_indices = np.arange(matrix.shape[0])[mask]\n",
    "\n",
    "    # Filter the column indices and values by the mask\n",
    "    col_indices = max_indices[mask]\n",
    "    data = max_values[mask]\n",
    "\n",
    "    # Create a CSR sparse matrix\n",
    "    sparse_matrix = csr_matrix((data.astype(np.float16), (row_indices, col_indices)), shape = matrix.shape)\n",
    "    \n",
    "    # Create a list of max values for each row, including zeros\n",
    "    max_values_per_row = np.where(mask, max_values, 1.25)\n",
    "\n",
    "    return sparse_matrix, max_values_per_row\n",
    "\n",
    "# Apply this to your cosine similarity matrices\n",
    "train_cosine_similarity_sparse, optimization_weights = keep_max_similarity_above_threshold_sparse(train_cosine_similarities)\n",
    "test_cosine_similarity_sparse, _ = keep_max_similarity_above_threshold_sparse(test_cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1a272",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T17:06:40.865744Z",
     "iopub.status.idle": "2023-12-30T17:06:40.866084Z",
     "shell.execute_reply": "2023-12-30T17:06:40.865931Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.865914Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOWERCASE = False\n",
    "VOCAB_SIZE = 42000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865202a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Creating Byte-Pair Encoding Tokenizer\n",
    "\n",
    "This cell initializes a Byte-Pair Encoding (BPE) tokenizer, a method effective for subword tokenization in NLP tasks. The tokenizer is configured with special tokens like `[UNK]`, `[PAD]`, `[CLS]`, `[SEP]`, and `[MASK]`. We use normalization and pre-tokenization strategies suitable for BPE. The tokenizer is trained on a subset of the dataset iteratively and wrapped in `PreTrainedTokenizerFast` for efficient tokenization. Finally, it's applied to both the test and training text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d1b41",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T17:06:40.867303Z",
     "iopub.status.idle": "2023-12-30T17:06:40.867646Z",
     "shell.execute_reply": "2023-12-30T17:06:40.86747Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.867455Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating Byte-Pair Encoding tokenizer\n",
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "dataset = Dataset.from_pandas(test[['text']])\n",
    "def train_corp_iter(): \n",
    "    for i in range(0, len(dataset), 25):\n",
    "        yield dataset[i : i + 25][\"text\"]\n",
    "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4ae3d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T17:06:40.869963Z",
     "iopub.status.idle": "2023-12-30T17:06:40.870425Z",
     "shell.execute_reply": "2023-12-30T17:06:40.870216Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.870194Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_texts_test = []\n",
    "\n",
    "for text in tqdm(test['text'].tolist()):\n",
    "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "\n",
    "tokenized_texts_train = []\n",
    "\n",
    "for text in tqdm(train['text'].tolist()):\n",
    "    tokenized_texts_train.append(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c958d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF Vectorization with Custom Tokenization\n",
    "\n",
    "This cell implements the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization process, customized for specific tokenization needs. The `TfidfVectorizer` is set up with a 3-5 n-gram range and various parameters including sublinear term frequency scaling and unicode accent stripping. Custom functions are used for both tokenization and preprocessing to maintain control over these processes. After fitting the vectorizer to the tokenized test data, we extract the vocabulary. This vocabulary is then used to initialize a new `TfidfVectorizer` which transforms both the training and test datasets. Post-processing, the vectorizer is deleted to free up memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e170a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T17:06:40.871838Z",
     "iopub.status.idle": "2023-12-30T17:06:40.872289Z",
     "shell.execute_reply": "2023-12-30T17:06:40.872079Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.872056Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dummy(text):\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
    "    tokenizer = dummy,\n",
    "    preprocessor = dummy,\n",
    "    token_pattern = None, strip_accents='unicode')\n",
    "\n",
    "vectorizer.fit(tokenized_texts_test)\n",
    "\n",
    "# Getting vocab\n",
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = dummy,\n",
    "                            preprocessor = dummy,\n",
    "                            token_pattern = None, strip_accents='unicode'\n",
    "                            )\n",
    "\n",
    "tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "tf_test = vectorizer.transform(tokenized_texts_test)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32088e8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T17:06:40.873699Z",
     "iopub.status.idle": "2023-12-30T17:06:40.87404Z",
     "shell.execute_reply": "2023-12-30T17:06:40.873886Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.87387Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#tf_train = hstack([tf_train, train_cosine_similarity_sparse])\n",
    "#tf_test = hstack([tf_test, test_cosine_similarity_sparse])\n",
    "\n",
    "del train_cosine_similarity_sparse, test_cosine_similarity_sparse\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410fa425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T17:06:40.899588Z",
     "iopub.status.busy": "2023-12-30T17:06:40.898876Z",
     "iopub.status.idle": "2023-12-30T17:06:40.903608Z",
     "shell.execute_reply": "2023-12-30T17:06:40.902845Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.899562Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = train['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69229e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Learning with Multiple Classifiers\n",
    "\n",
    "This cell sets up an ensemble learning model using various classifiers, each with its specific configurations:\n",
    "\n",
    "1. **Multinomial Naive Bayes**: \n",
    "   A `MultinomialNB` classifier with a smoothing parameter `alpha` set to 0.02.\n",
    "\n",
    "2. **SGD Classifier**: \n",
    "   An `SGDClassifier` for linear models with a modified Huber loss function, a maximum of 8000 iterations, and a tolerance of 1e-4 for stopping criteria.\n",
    "\n",
    "3. **LightGBM Classifier**: \n",
    "   An `LGBMClassifier` configured with custom parameters such as learning rate, lambda values, max depth, and more, specified in the `p6` dictionary.\n",
    "\n",
    "4. **CatBoost Classifier**: \n",
    "   A `CatBoostClassifier` with 1000 iterations, silent mode (no verbose output), specific learning rate, L2 regularization, and a subsampling rate of 0.4.\n",
    "\n",
    "5. **Ensemble Model - Voting Classifier**: \n",
    "   A `VotingClassifier` that combines the above models (`MultinomialNB`, `SGDClassifier`, `LGBMClassifier`, `CatBoostClassifier`) using soft voting. The weights for each classifier in the ensemble are specified, with a focus on the three non-Naive Bayes models.\n",
    "\n",
    "The ensemble model is then trained on the transformed training data (`tf_train`) and labels (`y_train`). Finally, the ensemble model is used to predict probabilities on the test dataset (`tf_test`), and garbage collection is run to manage memory.\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce826b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T17:06:40.920415Z",
     "iopub.status.busy": "2023-12-30T17:06:40.920156Z",
     "iopub.status.idle": "2023-12-30T17:06:40.926117Z",
     "shell.execute_reply": "2023-12-30T17:06:40.925051Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.920391Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clf = MultinomialNB(alpha=0.02)\n",
    "# sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n",
    "# p6={'n_iter': 2500,'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n",
    "#     'learning_rate': 0.00581909898961407, 'colsample_bytree': 0.78,\n",
    "#     'colsample_bynode': 0.8, 'lambda_l1': 4.562963348932286, \n",
    "#     'lambda_l2': 2.97485, 'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898}\n",
    "# lgb=LGBMClassifier(**p6)\n",
    "# cat=CatBoostClassifier(iterations=2500,\n",
    "#                        verbose=0,\n",
    "#                        l2_leaf_reg=6.6591278779517808,\n",
    "#                        learning_rate=0.005599066836106983,\n",
    "#                        subsample = 0.4,\n",
    "#                        allow_const_label=True,loss_function = 'CrossEntropy')\n",
    "# weights = [0.07,0.31,0.31,0.31]\n",
    "\n",
    "# ensemble = VotingClassifier(estimators=[('mnb',clf),\n",
    "#                                         ('sgd', sgd_model),\n",
    "#                                         ('lgb',lgb), \n",
    "#                                         #('cat', cat)\n",
    "#                                        ],\n",
    "#                             weights=weights, voting='soft', n_jobs=-1)\n",
    "# ensemble.fit(tf_train, y_train)\n",
    "# gc.collect()\n",
    "# final_preds_bpe = ensemble.predict_proba(tf_test)[:,1]\n",
    "\n",
    "# del tokenized_texts_test, tokenized_texts_train, dataset, raw_tokenizer, tokenizer\n",
    "# _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1c2ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T17:06:40.99471Z",
     "iopub.status.busy": "2023-12-30T17:06:40.994391Z",
     "iopub.status.idle": "2023-12-30T17:06:41.055916Z",
     "shell.execute_reply": "2023-12-30T17:06:41.051392Z",
     "shell.execute_reply.started": "2023-12-30T17:06:40.994684Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_voting(tf_train, tf_test, y_train, optimization_weights):\n",
    "    # Initialize classifiers\n",
    "    clf = MultinomialNB(alpha = 0.02)\n",
    "    sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n",
    "    p7={'n_iter': 1500,'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n",
    "    'learning_rate': 0.0058, 'colsample_bytree': 0.78,\n",
    "    'colsample_bynode': 0.8, 'lambda_l1': 4.563, \n",
    "    'lambda_l2': 2.97, 'min_data_in_leaf': 112, 'max_depth': 22, 'max_bin': 900}\n",
    "    lgb = LGBMClassifier(**p7)\n",
    "    cat=CatBoostClassifier(iterations=1000,\n",
    "                       verbose=0,\n",
    "                       l2_leaf_reg=6.659,\n",
    "                       learning_rate=0.0056,\n",
    "                       subsample = 0.4,\n",
    "                       allow_const_label=True,loss_function = 'CrossEntropy')\n",
    "    # Fit classifiers and make predictions\n",
    "    clf.fit(tf_train, y_train)\n",
    "    predictions_mnb = clf.predict_proba(tf_test)[:, 1]\n",
    "    del clf\n",
    "\n",
    "    sgd_model.fit(tf_train, y_train)\n",
    "    predictions_sgd = sgd_model.predict_proba(tf_test)[:, 1]\n",
    "    del sgd_model\n",
    "    \n",
    "    lgb.fit(tf_train, y_train, sample_weight = optimization_weights)\n",
    "    predictions_lgb = lgb.predict_proba(tf_test)[:, 1]\n",
    "    print('done with lightgbm')\n",
    "    del lgb\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    cat.fit(tf_train, y_train, sample_weight = optimization_weights)\n",
    "    predictions_cat = cat.predict_proba(tf_test)[:, 1]\n",
    "    print('done with catboost')\n",
    "    del cat\n",
    "    \n",
    "    # Define weights\n",
    "    weights = [0.07,0.31,0.31,0.31]\n",
    "\n",
    "    # Calculate weighted average of predictions\n",
    "    final_preds = (weights[0] * predictions_mnb + weights[1] * predictions_sgd + weights[2] * predictions_lgb + weights[3] * predictions_cat) / sum(weights)\n",
    "\n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    return final_preds\n",
    "final_preds_bpe = calculate_voting(tf_train, tf_test, y_train, optimization_weights)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd859dc5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Integrating Sentencepiece Encoding with Machine Learning Models\n",
    "\n",
    "## Tokenizer Setup\n",
    "1. **Sentencepiece Tokenizer Initialization**:\n",
    "   A `SentencePieceBPETokenizer` is initialized for subword tokenization.\n",
    "\n",
    "2. **Normalization and Pre-Tokenization**:\n",
    "   The tokenizer is configured with NFC normalization and optional lowercase conversion based on the `LOWERCASE` flag. Byte level pre-tokenization is also applied.\n",
    "\n",
    "3. **Special Tokens and Training**:\n",
    "   Special tokens like `[UNK]`, `[PAD]`, `[CLS]`, `[SEP]`, `[MASK]` are added. The tokenizer is trained on the test dataset using a custom generator function that iterates over the dataset in chunks.\n",
    "\n",
    "4. **Tokenization**:\n",
    "   The tokenizer processes both test and training datasets to create tokenized text data.\n",
    "\n",
    "## TF-IDF Vectorization\n",
    "1. **Vectorization Setup**:\n",
    "   A `TfidfVectorizer` with a 3-5 n-gram range and custom tokenizer and preprocessor (`dummy` function) is used. The vectorizer is first fitted to the tokenized test data.\n",
    "\n",
    "2. **Vocabulary Extraction and Transformation**:\n",
    "   The vocabulary from the test data vectorization is extracted and used to initialize a new `TfidfVectorizer`. This vectorizer then transforms both training and test tokenized texts.\n",
    "\n",
    "3. **Memory Management**:\n",
    "   The vectorizer is deleted, and garbage collection is run to manage memory.\n",
    "\n",
    "## Model Training and Prediction\n",
    "1. **Model Initialization**:\n",
    "   Multiple models including `MultinomialNB`, `SGDClassifier`, `LGBMClassifier`, and `CatBoostClassifier` are initialized with specific parameters.\n",
    "\n",
    "2. **Ensemble Model Creation**:\n",
    "   A `VotingClassifier` ensemble, using soft voting and specified weights, combines the aforementioned models.\n",
    "\n",
    "3. **Training and Prediction**:\n",
    "   The ensemble model is trained on the TF-IDF transformed training data and labels. It then predicts probabilities on the transformed test dataset.\n",
    "\n",
    "4. **Final Predictions and Cleanup**:\n",
    "   Predicted probabilities (`final_preds_spe`) are stored, and memory cleanup is performed with garbage collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aace4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T17:06:42.746821Z",
     "iopub.status.busy": "2023-12-30T17:06:42.74599Z",
     "iopub.status.idle": "2023-12-30T17:06:42.754425Z",
     "shell.execute_reply": "2023-12-30T17:06:42.753484Z",
     "shell.execute_reply.started": "2023-12-30T17:06:42.74679Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Creating Sentencepiece Encoding tokenizer\n",
    "# raw_tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "# # Adding normalization and pre_tokenizer\n",
    "# raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "# raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "# # Adding special tokens and creating trainer instance\n",
    "# special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "# # Creating huggingface dataset object\n",
    "# dataset = Dataset.from_pandas(test[['text']])\n",
    "\n",
    "# def train_corp_iter():\n",
    "#     \"\"\"\n",
    "#     A generator function for iterating over a dataset in chunks.\n",
    "#     \"\"\"    \n",
    "#     for i in range(0, len(dataset), 300):\n",
    "#         yield dataset[i : i + 300][\"text\"]\n",
    "\n",
    "# # Training from iterator REMEMBER it's training on test set...\n",
    "# raw_tokenizer.train_from_iterator(train_corp_iter())\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast(\n",
    "#     tokenizer_object = raw_tokenizer,\n",
    "#     unk_token=\"[UNK]\",\n",
    "#     pad_token=\"[PAD]\",\n",
    "#     cls_token=\"[CLS]\",\n",
    "#     sep_token=\"[SEP]\",\n",
    "#     mask_token=\"[MASK]\",\n",
    "# )\n",
    "\n",
    "# tokenized_texts_test = []\n",
    "\n",
    "# # Tokenize test set with new tokenizer\n",
    "# for text in tqdm(test['text'].tolist()):\n",
    "#     tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "\n",
    "# # Tokenize train set\n",
    "# tokenized_texts_train = []\n",
    "\n",
    "# for text in tqdm(train['text'].tolist()):\n",
    "#     tokenized_texts_train.append(tokenizer.tokenize(text))\n",
    "    \n",
    "# vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
    "#     tokenizer = dummy,\n",
    "#     preprocessor = dummy,\n",
    "#     token_pattern = None, strip_accents='unicode')\n",
    "\n",
    "# vectorizer.fit(tokenized_texts_test)\n",
    "\n",
    "# # Getting vocab\n",
    "# vocab = vectorizer.vocabulary_\n",
    "\n",
    "# print(vocab)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n",
    "#                             analyzer = 'word',\n",
    "#                             tokenizer = dummy,\n",
    "#                             preprocessor = dummy,\n",
    "#                             token_pattern = None, strip_accents='unicode'\n",
    "#                             )\n",
    "\n",
    "# tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "# tf_test = vectorizer.transform(tokenized_texts_test)\n",
    "\n",
    "# del vectorizer\n",
    "# gc.collect()\n",
    "\n",
    "# y_train = train['label'].values\n",
    "\n",
    "# def calculate_voting(tf_train, tf_test, y_train):\n",
    "#     # Initialize classifiers\n",
    "#     clf = MultinomialNB(alpha = 0.02)\n",
    "#     sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n",
    "#     p7={'n_iter': 1500,'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n",
    "#     'learning_rate': 0.0058, 'colsample_bytree': 0.78,\n",
    "#     'colsample_bynode': 0.8, 'lambda_l1': 4.563, \n",
    "#     'lambda_l2': 2.97, 'min_data_in_leaf': 112, 'max_depth': 22, 'max_bin': 900}\n",
    "#     lgb = LGBMClassifier(**p7)\n",
    "#     cat=CatBoostClassifier(iterations=1000,\n",
    "#                        verbose=0,\n",
    "#                        l2_leaf_reg=6.659,\n",
    "#                        learning_rate=0.0056,\n",
    "#                        subsample = 0.4,\n",
    "#                        allow_const_label=True,loss_function = 'CrossEntropy')\n",
    "#     # Fit classifiers and make predictions\n",
    "#     clf.fit(tf_train, y_train)\n",
    "#     predictions_mnb = clf.predict_proba(tf_test)[:, 1]\n",
    "#     del clf\n",
    "\n",
    "#     sgd_model.fit(tf_train, y_train)\n",
    "#     predictions_sgd = sgd_model.predict_proba(tf_test)[:, 1]\n",
    "#     del sgd_model\n",
    "    \n",
    "#     lgb.fit(tf_train, y_train)\n",
    "#     predictions_lgb = lgb.predict_proba(tf_test)[:, 1]\n",
    "#     print('done with lightgbm')\n",
    "#     del lgb\n",
    "#     _ = gc.collect()\n",
    "    \n",
    "#     cat.fit(tf_train, y_train)\n",
    "#     predictions_cat = cat.predict_proba(tf_test)[:, 1]\n",
    "#     print('done with catboost')\n",
    "#     del cat\n",
    "    \n",
    "#     # Define weights\n",
    "#     weights = [0.07,0.31,0.31,0.31]\n",
    "\n",
    "#     # Calculate weighted average of predictions\n",
    "#     final_preds = (weights[0] * predictions_mnb + weights[1] * predictions_sgd + weights[2] * predictions_lgb + weights[3] * predictions_cat) / sum(weights)\n",
    "\n",
    "#     # Garbage collection\n",
    "#     gc.collect()\n",
    "\n",
    "#     return final_preds\n",
    "# final_preds_spe = calculate_voting(tf_train, tf_test, y_train)\n",
    "# _ = gc.collect()\n",
    "# ###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ccdd1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Adjustments for type 1, 2 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e09513",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create a boolean mask for values greater than 0.5\n",
    "# mask = final_preds_bpe > 0.5\n",
    "\n",
    "# # Add 0.05 only to the values greater than 0.5\n",
    "# final_preds_bpe[mask] += 0.05\n",
    "\n",
    "# # Clip values to be between 0 and 1\n",
    "# final_preds_bpe = np.clip(final_preds_bpe, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0e7a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create a boolean mask for values less than 0.5\n",
    "# mask = final_preds_bpe < 0.5\n",
    "\n",
    "# # Subtract 0.05 only from the values less than 0.5\n",
    "# final_preds_bpe[mask] -= 0.05\n",
    "\n",
    "# # Clip values to be between 0 and 1\n",
    "# final_preds_bpe = np.clip(final_preds_bpe, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97533f14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "This section is for those who believe that their model has no idea when the probabilities are 0.45, 0.55, so you might as well score it as 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83638e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a boolean mask for values between 0.45 and 0.55\n",
    "mask = (final_preds_bpe >= 0.45) & (final_preds_bpe <= 0.55)\n",
    "\n",
    "# Set values in this range to 0.5\n",
    "final_preds_bpe[mask] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac920aa8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Final Submission and Closing Remarks\n",
    "\n",
    "## Submission Preparation\n",
    "In this final cell, we prepare our submission:\n",
    "\n",
    "1. **Ensemble Prediction Averaging**:\n",
    "   We combine the predictions from both the Byte-Pair Encoding (BPE) and Sentencepiece Encoding (SPE) models by averaging them. This approach helps in harnessing the strengths of both models.\n",
    "\n",
    "   ```python\n",
    "   sub['generated'] = (final_preds_bpe + final_preds_spe) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844694a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T17:06:44.063191Z",
     "iopub.status.busy": "2023-12-30T17:06:44.062585Z",
     "iopub.status.idle": "2023-12-30T17:06:44.100255Z",
     "shell.execute_reply": "2023-12-30T17:06:44.098979Z",
     "shell.execute_reply.started": "2023-12-30T17:06:44.063158Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign to the final solution\n",
    "sub['generated'] = final_preds_bpe\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42789226",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6888007,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 2740486,
     "sourceId": 4737381,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3945154,
     "sourceId": 6865136,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3960967,
     "sourceId": 6901341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3961875,
     "sourceId": 6971638,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.641229,
   "end_time": "2024-01-09T03:04:32.637982",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-09T03:04:14.996753",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "063ed8601ca142f3997623b887abc950": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0ecb1864402e4e27a45cccd80e9a588c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "135ecf6db59f4fcfbf004458dddd3cdb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1674e39f95ab4d0aa026774882a4c2ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "30ffdf2145f14378abfce9b1861c6729": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6770d30541244fe5af93320a252643d9",
       "placeholder": "​",
       "style": "IPY_MODEL_135ecf6db59f4fcfbf004458dddd3cdb",
       "value": " 44868/44868 [01:54&lt;00:00, 402.12it/s]"
      }
     },
     "3801336a7ebf436eb2752eaae78828ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4729b4d4672f4596b28e05941b027c3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0ecb1864402e4e27a45cccd80e9a588c",
       "placeholder": "​",
       "style": "IPY_MODEL_b26d5d45e05841c38af202817fc3e526",
       "value": "100%"
      }
     },
     "5846dcf105cf4fffa3147d8108774fbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6770d30541244fe5af93320a252643d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a37209fae5b4c119b58fcbf2dbcdbff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6debd6be2b7847cc8d7e589b67bdf6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9377b88216164716ade1e0abcd7ad2be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aba95a092e2c443eb393264858b4b415": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad4265db46aa4a37bec3ab2085d93746": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9377b88216164716ade1e0abcd7ad2be",
       "placeholder": "​",
       "style": "IPY_MODEL_b14f8c65a5ba4a20b085f3111d193855",
       "value": "100%"
      }
     },
     "b14f8c65a5ba4a20b085f3111d193855": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b19fe6c5f50e4eb7904b2003dec3cf91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_063ed8601ca142f3997623b887abc950",
       "max": 3,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1674e39f95ab4d0aa026774882a4c2ba",
       "value": 3
      }
     },
     "b26d5d45e05841c38af202817fc3e526": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c690dfd6956d4de3979900e558e0d9e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3801336a7ebf436eb2752eaae78828ed",
       "max": 44868,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6debd6be2b7847cc8d7e589b67bdf6bc",
       "value": 44868
      }
     },
     "cf75101cc21e45b3bb580ea746159d25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6a37209fae5b4c119b58fcbf2dbcdbff",
       "placeholder": "​",
       "style": "IPY_MODEL_5846dcf105cf4fffa3147d8108774fbe",
       "value": " 3/3 [00:00&lt;00:00, 228.02it/s]"
      }
     },
     "d086b0c9e7e240ee8cce0cc19353ccaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4729b4d4672f4596b28e05941b027c3b",
        "IPY_MODEL_c690dfd6956d4de3979900e558e0d9e2",
        "IPY_MODEL_30ffdf2145f14378abfce9b1861c6729"
       ],
       "layout": "IPY_MODEL_ffb417927aa34ac3a0afb0fefe809304"
      }
     },
     "e4221872af294ef4bc65efd12eec3810": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ad4265db46aa4a37bec3ab2085d93746",
        "IPY_MODEL_b19fe6c5f50e4eb7904b2003dec3cf91",
        "IPY_MODEL_cf75101cc21e45b3bb580ea746159d25"
       ],
       "layout": "IPY_MODEL_aba95a092e2c443eb393264858b4b415"
      }
     },
     "ffb417927aa34ac3a0afb0fefe809304": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
