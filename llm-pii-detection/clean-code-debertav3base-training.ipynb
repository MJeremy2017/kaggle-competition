{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":6865136,"sourceType":"datasetVersion","datasetId":3945154},{"sourceId":7466758,"sourceType":"datasetVersion","datasetId":4332496},{"sourceId":163088908,"sourceType":"kernelVersion"},{"sourceId":166389948,"sourceType":"kernelVersion"},{"sourceId":168928663,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Reference: \n1. https://www.kaggle.com/code/valentinwerner/915-deberta3base-training?scriptVersionId=161278765\n2. https://www.kaggle.com/code/kenjikonno/pii-distillbert-training\n3. https://huggingface.co/learn/nlp-course/en/chapter7/2\n4. [generate-data](https://www.kaggle.com/code/minhsienweng/create-ai-generated-essays-using-llm)\n\n\n\nTo dos:\n1. [done]use org train data to do train test split\n2. [done]add stride (stride on train and keep the validation same)\n3. [p1][done]build own tokenizer (4 epoch | larger vocab size)\n4. [done]use test tokenizer\n5. [done]generate more data\n6. similar data from test\n7. model ensembles\n8. [p1][done] inference stride\n9. [p1][done] match up 0.945 results\n10.[done] set truncation to [false](https://www.kaggle.com/code/ilya2raev/957-deberta3base-training).\n11. [p1]regex. Can detect URLs & ID Nums\n12.[x]add new tokens from training.\n13. [p1]match 0.962\n14. [done]v3-large with more data\n15. freeze 6 layers and flex embeddings\n\n\nURLs:\n\n1. http://www.burns-lopez.com/categories/appabout.asp\n2. http://jacobs-fisher.com/listpost.html\n3. https://www.youtube.com/watch?v=n-ajTPJ1h-J\n4. tps://www.facebook.com/bclark\n\n\nID-NUM:\n\n\n1.B-ID_NUM : 860632713425\n2. B-ID_NUM : 530670102508\n3. B-ID_NUM : 530670102508\n4. B-ID_NUM : Iz.:999893751750\n\n## Q&A\n\n__*Why the labels need to collate to have the same length for different docs?*__\n\nCollate to the same length in each batch.\n\n__*What's the difference between tokenizer with no truncation and tokenize with stride but truncated?*__\n\n\n__*Does training a new tokenizer help?*__\n\nThe pretrained model are closely tied with the pretrained tokenizer, as the `input_ids` of of each token will map to an embedding vector for model training. \n\nTraining a new tokenizer might disrupt the `input_ids`, the same word that maps to `input_ids=3`, after training might map to `input_ids=511`, which the model is unaware of, leading to misalignment. Normally it requires more fine-tunning with larger data set to accommodate this.","metadata":{}},{"cell_type":"markdown","source":"## Versions\n\n- __256train + 2048inference__: validation Recall: 0.81 | Precision: 0.88 | F1: 0.81 -> 0.664\n- __1024train + 2048inference + 3epoch__: validation Recall: 0.81 | Precision: 0.9 | F1: 0.82 -> 0.874\n- __stride(512, 214) + 512train + 2048inference + 2epoch__: validation Recall: 0.86 | Precision: 0.88 | F1: 0.867 -> 0.736\n- __stride(512, 214) + 1024train + 2048inference + 2epoch__: validation Recall: 0.84 | Precision: 0.88 | F1: 0.84 -> 0.862\n- __stride(700, 200) + 1024train + 2048inference + 2epoch__: validation Recall: 0.84 | Precision: 0.89 | F1: 0.84 -> 0.863\n- __stride(700, 200) + 1024train + 1024inference + 2epoch__: validation Recall: 0.84 | Precision: 0.89 | F1: 0.84 -> 0.82\n- __stride(700, 200) + 1024train + inference(1024, 400) + 2epoch__: validation Recall: 0.84 | Precision: 0.89 | F1: 0.84 -> 0.894\n- __stride(700, 200) + 1024train + inference(2048, 400) + 2epoch__: validation Recall: 0.84 | Precision: 0.89 | F1: 0.84 -> 0.896\n- __external_data + 1024train + inference(1024, 400) + 3epoch__: validation Recall: 0.991 | Precision: 0.983 | F1: 0.990 -> 0.945 (2048 infer does not work)\n- __external_data + stride(1024/700, 200) + 1024train + 1024inference + 3epoch__: validation Recall: 0.991 | Precision: 0.982 | F1: 0.990 -> 0.936 (1024 stride does not work)\n- __external_data + add_token + train(1024, 1024) + inference(1024, 400) + 3epoch__: validation Recall: 0.98 | Precision: 0.97 | F1: 0.98 -> 0.903\n- __external_data + add_test_token + train(1024) + inference(1024, 400) + 4epoch__: 0.884\n- __own_gen_data + external_data + train(1024) + inference(trunc=False) + 4epoch__: 0.951\n","metadata":{}},{"cell_type":"code","source":"!pip install seqeval -q\n\nimport json\nfrom itertools import chain\nfrom functools import partial\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom datasets import Dataset\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport random\nfrom seqeval.metrics import recall_score, precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:14:02.737361Z","iopub.execute_input":"2024-03-28T03:14:02.738279Z","iopub.status.idle":"2024-03-28T03:14:35.293037Z","shell.execute_reply.started":"2024-03-28T03:14:02.738236Z","shell.execute_reply":"2024-03-28T03:14:35.292066Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', '\n# B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n# 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n\n# cnt = 0\n# for _, row in data.iterrows():\n#     tok, label = row['tokens'], row['labels']\n#     for i, tk in enumerate(tok):\n#         lb: str = label[i]\n#         if lb.endswith(\"ID_NUM\"):\n#             print(lb, tk)\n#         if lb.endswith(\"URL_PERSONAL\"):\n#             print(lb, tk)\n#             cnt += 1\n#     if cnt >= 20:\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:35:19.621864Z","iopub.execute_input":"2024-03-28T03:35:19.622622Z","iopub.status.idle":"2024-03-28T03:35:19.827911Z","shell.execute_reply.started":"2024-03-28T03:35:19.622586Z","shell.execute_reply":"2024-03-28T03:35:19.826949Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"B-URL_PERSONAL https://www.jackson.com/list/explorehomepage.htm\nB-URL_PERSONAL https://www.linkedin.com/in/mmartinez\nB-URL_PERSONAL https://youtu.be/rFD2lJuvace\nB-ID_NUM 860632713425\nB-ID_NUM 530670102508\nB-ID_NUM 530670102508\nB-ID_NUM 875673967537\nB-ID_NUM 860632713425\nB-ID_NUM 557349702179\nB-ID_NUM 784372734211\nB-ID_NUM 054176622314\nB-ID_NUM 674915248960\nB-URL_PERSONAL https://www.hall.biz/wp-contenthome.html\nB-URL_PERSONAL http://www.burns-lopez.com/categories/appabout.asp\nB-URL_PERSONAL http://jacobs-fisher.com/listpost.html\nB-URL_PERSONAL https://www.youtube.com/watch?v=n-ajTPJ1h-J\nB-ID_NUM 932353568953\nB-URL_PERSONAL tps://www.facebook.com/bclark\nB-URL_PERSONAL https://www.youtube.com/channel/UC1ElAcppeuhfet\nI-URL_PERSONAL nYZqnhEXw\nB-URL_PERSONAL https://oconnell-townsend.com/wp-content/categorieshomepage.html\nB-ID_NUM 982645662261\nB-ID_NUM 409046248321\nB-URL_PERSONAL https://www.peterson.net/tag/app/listmain.php\nB-URL_PERSONAL https://diaz.com/tag/wp-contentlogin.htm\nB-URL_PERSONAL https://diaz.com/tag/wp-contentlogin.htm\nB-URL_PERSONAL https://diaz.com/tag/wp-contentlogin.htm\nB-URL_PERSONAL https://diaz.com/tag/wp-contentlogin.htm\nB-URL_PERSONAL http://hodge-ramsey.com/tagmain.html\nB-URL_PERSONAL http://jones-mendoza.com/blog/search/searchprivacy.php\nB-URL_PERSONAL https://www.kramer.info/wp-content/category/bloghome.php\nB-ID_NUM 163133980712\nB-ID_NUM 186941941714\nB-ID_NUM 159531167997\nB-ID_NUM 159531167997\nB-ID_NUM 046922558887\nB-ID_NUM 943063077874\nB-ID_NUM 792389774673\nB-ID_NUM 167695383458\nB-ID_NUM Iz.:999893751750\nB-ID_NUM Kl.:838901042770\nB-ID_NUM 06EYD876\nB-URL_PERSONAL http://www.chambers.com/wp-content/app/categorieshomepage.jsp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    seed = 2024\n    offline_validation = False\n    submit = True\n    \n    origin_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n    external_path1 = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"\n    external_path2 = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"\n    gen_path1 = \"/kaggle/input/create-ai-generated-essays-using-llm/pii_dataset_Gemma.json\"\n    gen_path2 = \"/kaggle/input/k/zhangyue199/create-ai-generated-essays-using-llm/pii_dataset_Gemma.json\"\n\n\n    TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\n#     TRAINING_MODEL_PATH = \"/kaggle/input/save-debertav3-base\" + \"/deberta-v3-base\"\n    TRAINING_MAX_LENGTH = 1024\n    TRAINING_EPOCH = 5\n    \n    STRIDE_MAX_LENTH = 1024\n    STRIDE_STEP = 400\n    \n    TRAIN_TOKENIZER = False\n    VOCAB_SIZE = 30000\n    \n    \ndef seed_everything(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n\nseed_everything(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:14:35.294552Z","iopub.execute_input":"2024-03-28T03:14:35.295124Z","iopub.status.idle":"2024-03-28T03:14:35.301781Z","shell.execute_reply.started":"2024-03-28T03:14:35.295097Z","shell.execute_reply":"2024-03-28T03:14:35.300943Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## import dataset and downsample","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:04:16.339305Z","iopub.execute_input":"2024-02-29T03:04:16.339846Z","iopub.status.idle":"2024-02-29T03:04:16.376552Z","shell.execute_reply.started":"2024-02-29T03:04:16.339820Z","shell.execute_reply":"2024-02-29T03:04:16.375535Z"}}},{"cell_type":"code","source":"def read_json(path):\n    return json.load(open(path))\n\n\ndef count_negative_positive_doc(data):\n    p = n = 0\n    positive = []\n    negative = [] \n    for d in tqdm(data, desc=\"preprocess data\"):\n        # some corrupted data\n        if type(d) != dict:\n            continue\n        if any(np.array(d[\"labels\"]) != \"O\"): \n            p += 1\n            positive.append(d)\n        else:\n            n += 1\n            negative.append(d)\n    print(f\"positive samples {p} | negative samples {n}\")\n    return positive, negative\n    \n\n# quantile 0.5: 700 | 0.75: 900 | 0.9: 1100\n# data = pd.DataFrame(read_json(Config.origin_path))  # positive samples 945/6807\ndata_org = read_json(Config.origin_path)\npos, neg = count_negative_positive_doc(data_org)\ndata_ext1 = read_json(Config.external_path1)\ndata_ext2 = read_json(Config.external_path2)\n\ngen1 = pd.DataFrame(read_json(Config.gen_path1))\ngen2 = pd.DataFrame(read_json(Config.gen_path2))\n# gen = pd.concat([gen1, gen2, gen2], axis=0).to_dict(orient='records')\n# positive samples 5568 | negative samples 911\n\n\nprint(\"size data origin\", len(data_org), \"data external1\", len(data_ext1), \n      \"data external2\", len(data_ext2), \"gemma generated: \", len(gen1) + len(gen2) + len(gen2))\n\ndata1 = pd.DataFrame(pos + neg[:len(neg)//3] + data_ext1 + data_ext2)\ndata = pd.concat([data1, gen1, gen2, gen2], axis=0)\ndata['document'] = data['document'].astype(str)\ndata.reset_index(drop=True, inplace=True)\n# data = pd.DataFrame(data_org)\nprint(\"merged size\", len(data))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:24:32.207308Z","iopub.execute_input":"2024-03-28T03:24:32.207681Z","iopub.status.idle":"2024-03-28T03:24:37.633781Z","shell.execute_reply.started":"2024-03-28T03:24:32.207653Z","shell.execute_reply":"2024-03-28T03:24:37.632819Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"preprocess data: 100%|██████████| 6807/6807 [00:01<00:00, 4008.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"positive samples 945 | negative samples 5862\nsize data origin 6807 data external1 4434 data external2 2000 gemma generated:  6479\nmerged size 15812\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.histplot(data['tokens'].apply(lambda x: len(x)))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:12.478545Z","iopub.execute_input":"2024-03-20T04:03:12.478830Z","iopub.status.idle":"2024-03-20T04:03:13.217371Z","shell.execute_reply.started":"2024-03-20T04:03:12.478804Z","shell.execute_reply":"2024-03-20T04:03:13.216468Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='tokens', ylabel='Count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Define Validation Set","metadata":{}},{"cell_type":"code","source":"def encode_labels(data):\n    data['unique_labels'] = data['labels'].apply(lambda x: set(l.split('-')[1] if l != 'O' else 'O' for l in x))\n    mlb = MultiLabelBinarizer()\n    \n    arr = mlb.fit_transform(data['unique_labels'])\n    label_df = pd.DataFrame(arr, columns=mlb.classes_)\n    return pd.concat([data, label_df], axis=1), mlb.classes_\n\n\ndef count_labels(data, labels):\n    label_cnt = dict()\n    for label in c:\n        doc_cnt = data[label].sum()\n        label_cnt[label] = doc_cnt\n        print(f\"{label}: {doc_cnt}\")\n    return label_cnt","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:13.218512Z","iopub.execute_input":"2024-03-20T04:03:13.218766Z","iopub.status.idle":"2024-03-20T04:03:13.225725Z","shell.execute_reply.started":"2024-03-20T04:03:13.218743Z","shell.execute_reply":"2024-03-20T04:03:13.224820Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"labeled_data, c = encode_labels(data)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:13.226701Z","iopub.execute_input":"2024-03-20T04:03:13.227013Z","iopub.status.idle":"2024-03-20T04:03:14.064632Z","shell.execute_reply.started":"2024-03-20T04:03:13.226980Z","shell.execute_reply":"2024-03-20T04:03:14.063764Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"Data size\", labeled_data.shape)\nlabel_cnt = count_labels(labeled_data, c)\n    \nVAL_RATE = 0.1\nlabeled_data['validation'] = 0\nfor label, cnt in label_cnt.items():\n    sample_size = int(cnt * VAL_RATE)\n    ind = labeled_data[labeled_data[label] == 1].sample(sample_size, random_state=Config.seed).index\n    labeled_data.loc[ind, 'validation'] = 1\n\nif Config.offline_validation:\n    val_data = labeled_data[labeled_data['validation'] == 1]\n    train_data = labeled_data[labeled_data['validation'] == 0]\n    print(\"\\nValidation set size\", val_data.shape, 'Train set size', train_data.shape)\n    _ = count_labels(val_data, c)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:14.065765Z","iopub.execute_input":"2024-03-20T04:03:14.066095Z","iopub.status.idle":"2024-03-20T04:03:14.106362Z","shell.execute_reply.started":"2024-03-20T04:03:14.066068Z","shell.execute_reply":"2024-03-20T04:03:14.105396Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Data size (14812, 14)\nEMAIL: 6393\nID_NUM: 2808\nNAME_STUDENT: 8534\nO: 14812\nPHONE_NUM: 7454\nSTREET_ADDRESS: 8404\nURL_PERSONAL: 2686\nUSERNAME: 3741\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Stride","metadata":{}},{"cell_type":"code","source":"def split_stride(df: pd.DataFrame, max_length, step):\n    # no stride\n    return df\n    new_df = []\n    for _, row in df.iterrows():\n        token_len = len(row['tokens'])\n        if token_len > max_length:\n            start = 0\n            while start < token_len:\n                if start + max_length > token_len:\n                    new_row = {\n                        'document': row['document'],\n                        'full_text': \"\",\n                        'tokens': row['tokens'][token_len-max_length:token_len],\n                        'trailing_whitespace': row['trailing_whitespace'][token_len-max_length:token_len],\n                        'labels': row['labels'][token_len-max_length:token_len],\n                    }\n                    start += max_length\n                else:\n                    new_row = {\n                        'document': row['document'],\n                        'full_text': \"\",\n                        'tokens': row['tokens'][start:start+max_length],\n                        'trailing_whitespace': row['trailing_whitespace'][start:start+max_length],\n                        'labels': row['labels'][start:start+max_length],\n                    }\n                    start += step\n                new_df.append(new_row)\n        else:\n            new_row = {\n                'document': row['document'],\n                'full_text': row['full_text'],\n                'tokens': row['tokens'],\n                'trailing_whitespace': row['trailing_whitespace'],\n                'labels': row['labels'],\n            }\n            new_df.append(new_row)\n    return pd.DataFrame(new_df)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:14.107693Z","iopub.execute_input":"2024-03-20T04:03:14.108159Z","iopub.status.idle":"2024-03-20T04:03:14.118846Z","shell.execute_reply.started":"2024-03-20T04:03:14.108110Z","shell.execute_reply":"2024-03-20T04:03:14.117867Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Split stride only on train data so that we use the same validation set for reference\nif Config.offline_validation:\n    print(\"Train size\", train_data.shape)\n    train_data_stride = split_stride(train_data, Config.STRIDE_MAX_LENTH, Config.STRIDE_STEP)\n    print(\"After striding size\", train_data_stride.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:09:12.691058Z","iopub.execute_input":"2024-03-20T04:09:12.691431Z","iopub.status.idle":"2024-03-20T04:09:12.699424Z","shell.execute_reply.started":"2024-03-20T04:09:12.691399Z","shell.execute_reply":"2024-03-20T04:09:12.698412Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(\"Whole data size\", labeled_data.shape)\nlabeled_data_stride = split_stride(labeled_data, Config.STRIDE_MAX_LENTH, Config.STRIDE_STEP)\nprint(\"After striding size\", labeled_data_stride.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:09:14.234252Z","iopub.execute_input":"2024-03-20T04:09:14.234986Z","iopub.status.idle":"2024-03-20T04:09:14.239811Z","shell.execute_reply.started":"2024-03-20T04:09:14.234955Z","shell.execute_reply":"2024-03-20T04:09:14.238934Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Whole data size (14812, 15)\nAfter striding size (14812, 15)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### labels","metadata":{}},{"cell_type":"code","source":"# * unpack the list of labels and chain together\nall_labels = sorted(list(set(chain(*[x for x in data.labels]))))\nprint(f\"all_labels:{all_labels}\")\n\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:09:16.654702Z","iopub.execute_input":"2024-03-20T04:09:16.655517Z","iopub.status.idle":"2024-03-20T04:09:16.780131Z","shell.execute_reply.started":"2024-03-20T04:09:16.655483Z","shell.execute_reply":"2024-03-20T04:09:16.779119Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"all_labels:['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"txt = \"This isn't an example\"\ntokenizer = AutoTokenizer.from_pretrained(Config.TRAINING_MODEL_PATH)\nprint(\"Old tokenizer:\\n\", tokenizer.tokenize(txt))\n\nif Config.TRAIN_TOKENIZER:\n    # Use only the original data for training tokenizer\n    text = [x['full_text'] for x in data_org]\n    data_corpus = (text[i:i+1000] for i in range(0, len(text), 1000))\n    tokenizer_new = tokenizer.train_new_from_iterator(data_corpus, vocab_size=Config.VOCAB_SIZE)\n    print(\"New tokenizer:\\n\", tokenizer_new.tokenize(txt))\n\n    # Add new tokens to existing vocab\n    tokens_new = set()\n    for tk in tokenizer_new.vocab.keys():\n        if tokenizer.convert_tokens_to_ids(tk) == tokenizer.unk_token_id:\n            tokens_new.add(tk)\n\n    print(\"Added unknown tokens\", len(tokens_new))\n    tokenizer.add_tokens(list(tokens_new))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:09:17.593304Z","iopub.execute_input":"2024-03-20T04:09:17.593690Z","iopub.status.idle":"2024-03-20T04:09:20.037183Z","shell.execute_reply.started":"2024-03-20T04:09:17.593658Z","shell.execute_reply":"2024-03-20T04:09:20.036167Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc20bfcaa674aca9e20a639d588e8e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09bc6819491e4076a696ce8aed8c1b02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4746c8ed9bc425b950692526e1e5ce5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Old tokenizer:\n ['▁This', '▁isn', \"'\", 't', '▁an', '▁example']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Offline Validation","metadata":{}},{"cell_type":"markdown","source":"## Tokenizer\n\nThe tokenizer returns multiple keys\n```\ntokenizer(text, return_offsets_mapping=True, max_length=max_length)\n```\n\n1. __input_ids__: The `id` of the token in the vocabulary.\n2. __token_type_ids__: This is used in question answering training, where the question sequence will be marked as `0` and answer sequence with `1`.\n3. __attention_mask__: Tells the model which tokens to attend to, the original tokens will be marked `1`, the padded will be marked `0`.\n4. __offset_mapping__: a tuple of `(start_idx, end_idx)` corresponds to the token position. `(0, 0)` indicates the start and end.\n\nResults with `max_length` number of tokens will be returned, for sentence shorter than it will be padded, longer ones will be truncated.","metadata":{}},{"cell_type":"code","source":"# This tokenize the sentence and align tokenized token with a label derived from the original label.\ndef tokenize(doc, tokenizer, label2id, max_length):\n    text = []\n    labels = []\n    \n    # give each char a label\n    for t, l, ws in zip(doc[\"tokens\"], doc[\"provided_labels\"], doc[\"trailing_whitespace\"]):\n        text.append(t)\n        labels.extend([l] * len(t))\n\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n\n    text = \"\".join(text)\n    labels = np.array(labels)\n     \n    tokenized = tokenizer(text, return_offsets_mapping=True, max_length=max_length)\n#     tokenized = tokenizer(text, return_offsets_mapping=True, truncation=False)\n    token_labels = []\n    for start_idx, end_idx in tokenized.offset_mapping:\n        # start token\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        if text[start_idx].isspace():\n            start_idx += 1\n        # TODO: maybe for a token that splits into multiple ones, the later ones should be I-Class?\n        token_labels.append(label2id[labels[start_idx]])\n\n    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}\n\n\ndef tokenize_data_from_pandas(data, tokenizer):\n    data['provided_labels'] = data['labels']\n    ds = Dataset.from_pandas(data)\n\n    ds = ds.map(tokenize,\n                fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": Config.TRAINING_MAX_LENGTH}, \n                num_proc=2)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:09:20.038831Z","iopub.execute_input":"2024-03-20T04:09:20.039176Z","iopub.status.idle":"2024-03-20T04:09:20.049329Z","shell.execute_reply.started":"2024-03-20T04:09:20.039149Z","shell.execute_reply":"2024-03-20T04:09:20.048255Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(Config.TRAINING_MODEL_PATH)\nif Config.offline_validation:\n    print(\"Tokenize train data ...\")\n    # Here uses strided train data\n    ds_train = tokenize_data_from_pandas(train_data_stride, tokenizer)\n    print(\"Tokenize validation data ...\")\n    ds_val = tokenize_data_from_pandas(val_data, tokenizer)\n    \nif Config.submit:\n    print(\"Tokenize whole data ...\")\n    ds = tokenize_data_from_pandas(labeled_data, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:09:20.050517Z","iopub.execute_input":"2024-03-20T04:09:20.050785Z","iopub.status.idle":"2024-03-20T04:12:13.677367Z","shell.execute_reply.started":"2024-03-20T04:09:20.050761Z","shell.execute_reply":"2024-03-20T04:12:13.676230Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Tokenize whole data ...\n   ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/7406 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce145004b6b54a55931fb0fed360cd94"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/7406 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8a1f2100adf4febb4cf6b171747dd7e"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Metric","metadata":{}},{"cell_type":"code","source":"def compute_metrics(p, all_labels):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f_score\n    }\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:15.530688Z","iopub.status.idle":"2024-03-20T04:03:15.531163Z","shell.execute_reply.started":"2024-03-20T04:03:15.530933Z","shell.execute_reply":"2024-03-20T04:03:15.530951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model and parameter","metadata":{}},{"cell_type":"code","source":"# classification model\ndef init_model():\n    model = AutoModelForTokenClassification.from_pretrained(\n        Config.TRAINING_MODEL_PATH,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id,\n        ignore_mismatched_sizes=True,\n    )\n    # for adding new tokenizer\n    if Config.TRAIN_TOKENIZER:\n        model.resize_token_embeddings(len(tokenizer))\n    return model\n\n# collate the individual data samples and pad to the same length in each batch\n# pad_to_multiple_of padding strategy is useful for training on NVIDIA cores\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:15.532106Z","iopub.status.idle":"2024-03-20T04:03:15.532472Z","shell.execute_reply.started":"2024-03-20T04:03:15.532298Z","shell.execute_reply":"2024-03-20T04:03:15.532314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Arguments\n\n- fp16: this enables mixed-precision training, which can reduce memory usage and speed up training, provided that your hardware supports it (e.g., modern NVIDIA GPUs with Tensor Cores).\n- learning_rate: The initial learning rate for the optimizer. The value 5e-5 is a common starting point for many NLP tasks and models.\n- num_train_epochs: The total number of training epochs to run. __An epoch is a complete pass through the entire training dataset.__\n\n- per_device_train_batch_size: The batch size per device during training. If you're using multiple GPUs, each will process this many examples per step.\n\n- gradient_accumulation_steps: This parameter allows you to accumulate gradients over multiple steps before performing an optimization step. It effectively increases the batch size without increasing the memory requirement.\n\n- evaluation_strategy: Determines when evaluation is performed if do_eval is set to True. Options are \"steps\", \"no\", \"epoch\".\n\n- save_total_limit: The maximum number of model checkpoints to save. Older checkpoints are deleted to save space if this limit is exceeded. Setting it to 1 means only the most recent checkpoint is kept.\n\n- logging_steps: The number of training steps between logging events. It controls how often progress logs are printed out.\n\n- lr_scheduler_type: Specifies the type of learning rate scheduler to use. The 'cosine' scheduler type adjusts the learning rate based on a cosine curve, which can lead to better performance by smoothly decreasing the learning rate.\n\n- metric_for_best_model: This parameter is used to identify the best model during training when evaluation is performed. It specifies which metric to use for this purpose. In this case, \"f1\" suggests that the F1 score is used to evaluate model performance.\n\n- greater_is_better: Indicates whether a higher value of the metric_for_best_model is better. Setting it to True means that higher F1 scores indicate better models.\n\n- warmup_ratio: The proportion of training steps to perform linear learning rate warm-up. For example, __0.1 means the learning rate will gradually increase over the first 10% of the total training steps__ before following the specified scheduler.\n\n- weight_decay: This adds a L2 penalty to the cost, which can help prevent the model from overfitting on the training data. A value of 0.01 applies a small amount of regularization.\n","metadata":{}},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"%%time\nif Config.offline_validation:\n    print(\"Offline validation\")\n    model = init_model()\n    args = TrainingArguments(\n        output_dir=\"output\", \n        fp16=True,\n        learning_rate=5e-5,\n        num_train_epochs=Config.TRAINING_EPOCH,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=2,\n        report_to=\"none\",\n        evaluation_strategy=\"steps\",\n        do_eval=True,\n        eval_steps=500,\n        save_total_limit=1,\n        logging_steps=50,\n        lr_scheduler_type='cosine',\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        warmup_ratio=0.1,\n        weight_decay=0.01  # L2 reg\n    )\n    trainer = Trainer(\n        model=model, \n        args=args, \n        train_dataset=ds_train,\n        eval_dataset=ds_val,\n        data_collator=collator, \n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, all_labels=all_labels),\n    )\n\n    # Recall Precision F1 are from validation set\n    trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:15.533812Z","iopub.status.idle":"2024-03-20T04:03:15.534181Z","shell.execute_reply.started":"2024-03-20T04:03:15.534008Z","shell.execute_reply":"2024-03-20T04:03:15.534035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Full Training","metadata":{}},{"cell_type":"code","source":"if Config.submit:\n    print(\"Online submission\")\n    model = init_model()\n    args = TrainingArguments(\n        output_dir=\"output\", \n        fp16=True,\n        learning_rate=2e-5, # use lower lr\n        num_train_epochs=Config.TRAINING_EPOCH,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=2,\n        report_to=\"none\",\n        evaluation_strategy=\"no\",\n        do_eval=True,\n        save_total_limit=1,\n        logging_steps=200,\n        lr_scheduler_type='cosine',\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        warmup_ratio=0.1,\n        weight_decay=0.01\n    )\n\n    trainer = Trainer(\n        model=model, \n        args=args, \n        train_dataset=ds,\n        data_collator=collator, \n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, all_labels=all_labels),\n    )\n\n    trainer.train()\n    \n    print(\"Saving model ...\")\n    name = f\"debertaV3_len{Config.TRAINING_MAX_LENGTH}_epoch{Config.TRAINING_EPOCH}\"\n    trainer.save_model(name)\n    tokenizer.save_pretrained(name)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T04:03:15.535851Z","iopub.status.idle":"2024-03-20T04:03:15.536322Z","shell.execute_reply.started":"2024-03-20T04:03:15.536088Z","shell.execute_reply":"2024-03-20T04:03:15.536109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}