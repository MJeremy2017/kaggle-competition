{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7110479,"sourceType":"datasetVersion","datasetId":4099711},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421}],"dockerImageVersionId":30588,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Meta Classifier\n\nThe idea is to use another model to fit the predictions instead of a simple voting classifier.","metadata":{}},{"cell_type":"code","source":"import sys\nimport gc\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport pandas as pd\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\nfrom tqdm.notebook import tqdm\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier","metadata":{"execution":{"iopub.status.busy":"2024-01-13T07:06:40.554720Z","iopub.execute_input":"2024-01-13T07:06:40.555585Z","iopub.status.idle":"2024-01-13T07:06:40.563159Z","shell.execute_reply.started":"2024-01-13T07:06:40.555538Z","shell.execute_reply":"2024-01-13T07:06:40.562228Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"LGBM Parameters taken from : https://www.kaggle.com/code/siddhvr/llm-daigt-sub","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\norg_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:33:11.502474Z","iopub.execute_input":"2024-01-13T06:33:11.503067Z","iopub.status.idle":"2024-01-13T06:33:13.641602Z","shell.execute_reply.started":"2024-01-13T06:33:11.503040Z","shell.execute_reply":"2024-01-13T06:33:13.640559Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:33:13.642953Z","iopub.execute_input":"2024-01-13T06:33:13.643331Z","iopub.status.idle":"2024-01-13T06:33:13.717697Z","shell.execute_reply.started":"2024-01-13T06:33:13.643295Z","shell.execute_reply":"2024-01-13T06:33:13.716846Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 15000000\nOFFLINE = False","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:33:13.719983Z","iopub.execute_input":"2024-01-13T06:33:13.720742Z","iopub.status.idle":"2024-01-13T06:33:13.728301Z","shell.execute_reply.started":"2024-01-13T06:33:13.720715Z","shell.execute_reply":"2024-01-13T06:33:13.727562Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\ndataset = Dataset.from_pandas(test[['text']])\ndef train_corp_iter(): \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:33:13.729297Z","iopub.execute_input":"2024-01-13T06:33:13.729574Z","iopub.status.idle":"2024-01-13T06:35:10.186256Z","shell.execute_reply.started":"2024-01-13T06:33:13.729551Z","shell.execute_reply":"2024-01-13T06:35:10.185308Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e4cd68041df447f9699afd7a31504d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/44868 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dfd894f3ba9450ea0e117235afaa806"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_texts_test[1]","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:35:10.187455Z","iopub.execute_input":"2024-01-13T06:35:10.187742Z","iopub.status.idle":"2024-01-13T06:35:10.194431Z","shell.execute_reply.started":"2024-01-13T06:35:10.187717Z","shell.execute_reply":"2024-01-13T06:35:10.193502Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['ĠBbb', 'Ġccc', 'Ġddd', '.']"},"metadata":{}}]},{"cell_type":"code","source":"def dummy(text):\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:35:10.195568Z","iopub.execute_input":"2024-01-13T06:35:10.195841Z","iopub.status.idle":"2024-01-13T06:35:10.206455Z","shell.execute_reply.started":"2024-01-13T06:35:10.195818Z","shell.execute_reply":"2024-01-13T06:35:10.205484Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%time\nvectorizer = TfidfVectorizer(\n    ngram_range=(3, 5), \n    lowercase=False, \n    sublinear_tf=True, \n    analyzer='word',\n    tokenizer=dummy,\n    preprocessor=dummy,\n    token_pattern=None, \n    strip_accents='unicode')\n\nvectorizer.fit(tokenized_texts_test)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\n\nprint(vocab)\n\nvectorizer = TfidfVectorizer(\n    ngram_range=(3, 5), \n    lowercase=False, \n    sublinear_tf=True, \n    vocabulary=vocab,\n    analyzer='word',\n    tokenizer=dummy,\n    preprocessor=dummy,\n    token_pattern=None, \n    strip_accents='unicode'\n)\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:35:10.207627Z","iopub.execute_input":"2024-01-13T06:35:10.207902Z","iopub.status.idle":"2024-01-13T06:39:16.778659Z","shell.execute_reply.started":"2024-01-13T06:35:10.207880Z","shell.execute_reply":"2024-01-13T06:39:16.777777Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'ĠAaa Ġbbb Ġccc': 0, 'Ġbbb Ġccc .': 6, 'ĠAaa Ġbbb Ġccc .': 1, 'ĠBbb Ġccc Ġddd': 2, 'Ġccc Ġddd .': 7, 'ĠBbb Ġccc Ġddd .': 3, 'ĠCCC Ġddd Ġeee': 4, 'Ġddd Ġeee .': 8, 'ĠCCC Ġddd Ġeee .': 5}\nCPU times: user 4min 6s, sys: 153 ms, total: 4min 6s\nWall time: 4min 6s\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"51"},"metadata":{}}]},{"cell_type":"code","source":"y_train = train['label'].values","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:39:16.779869Z","iopub.execute_input":"2024-01-13T06:39:16.780180Z","iopub.status.idle":"2024-01-13T06:39:16.784996Z","shell.execute_reply.started":"2024-01-13T06:39:16.780154Z","shell.execute_reply":"2024-01-13T06:39:16.783952Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Cohesion Features V2","metadata":{}},{"cell_type":"code","source":"# def word_diversity(text):\n#     tk = word_tokenize(text)\n#     return len(set(tk)) / len(tk)\n\n\n# def sentence_length(text):\n#     tk = sent_tokenize(text)\n#     return [len(s) for s in tk]\n\n\n# def generate_cohesion_features(data):\n#     features = []\n#     data['word_diversity'] = data['text'].apply(lambda x: word_diversity(x))\n#     data['sentence_len'] = data['text'].apply(lambda x: sentence_length(x))\n#     data['sentence_cnt'] = data['sentence_len'].apply(lambda x: len(x))\n#     features.extend(['word_diversity', 'sentence_cnt'])\n    \n#     ops = [('mean', np.mean), ('max', np.max), ('min', np.min), ('median', np.median)]\n#     for op, func in ops:\n#         col = f'sentence_{op}'\n#         data[col] = data['sentence_len'].apply(lambda x: func(x))\n#         features.append(col)\n#     return data[features]","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:39:16.788561Z","iopub.execute_input":"2024-01-13T06:39:16.789244Z","iopub.status.idle":"2024-01-13T06:39:16.797916Z","shell.execute_reply.started":"2024-01-13T06:39:16.789211Z","shell.execute_reply":"2024-01-13T06:39:16.796983Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# %%time\n# train_co = generate_cohesion_features(train)\n# test_co = generate_cohesion_features(test)\n\n# print('train shape', train_co.shape, train_co.sample(3))","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:39:16.798868Z","iopub.execute_input":"2024-01-13T06:39:16.799158Z","iopub.status.idle":"2024-01-13T06:39:16.811667Z","shell.execute_reply.started":"2024-01-13T06:39:16.799132Z","shell.execute_reply":"2024-01-13T06:39:16.810935Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# if OFFLINE:\n#     print(\"Training offline\")\n#     skf = StratifiedKFold(3)\n#     scores = dict()\n\n#     for i, (train_idx, val_idx) in enumerate(skf.split(train_co, y_train)):\n#         train_X_, train_y_ = train_co.iloc[train_idx], y_train[train_idx]\n#         val_X_, val_y_ = train_co.iloc[val_idx], y_train[val_idx]\n#         print(f\"Fold {i+1}: train shape {train_X_.shape} | val shape {val_X_.shape}\")\n\n#         clf = LGBMClassifier()\n#         clf.fit(train_X_, train_y_)\n#         pred_ = clf.predict_proba(val_X_)[:, 1]\n\n#         auc = roc_auc_score(val_y_, pred_)\n#         scores[f\"fold_{i+1}\"] = auc\n#         print(f\"Fold {i+1} auc {auc}\")\n\n#     mean_auc = np.mean(list(scores.values()))\n#     print(\"AUC on training set is\", mean_auc)\n# else:\n#     print(\"Training full online\")\n#     clf = LGBMClassifier()\n#     clf.fit(train_co, y_train)\n#     pred_cohesion = clf.predict_proba(test_co)[:, 1]\n#     print(pred_cohesion)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:39:16.812682Z","iopub.execute_input":"2024-01-13T06:39:16.812912Z","iopub.status.idle":"2024-01-13T06:39:16.821662Z","shell.execute_reply.started":"2024-01-13T06:39:16.812892Z","shell.execute_reply":"2024-01-13T06:39:16.820905Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Filter Train data Based on Test Stats","metadata":{}},{"cell_type":"code","source":"# # Filter by word diversity and sentence count\n# wd_min = test_co['word_diversity'].min()\n# wd_max = test_co['word_diversity'].max()\n# sc_min = test_co['sentence_cnt'].min()\n# sc_max = test_co['sentence_cnt'].max()\n\n# cond_ = (wd_min <= train_co['word_diversity']) & (train_co['word_diversity'] <= wd_max) & \\\n#         (sc_min <= train_co['sentence_cnt']) & (train_co['sentence_cnt'] <= sc_max)\n\n# idx = train_co[cond_].index\n# tf_train = tf_train[idx]\n# print(\"Filtered training set\", tf_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:39:16.822679Z","iopub.execute_input":"2024-01-13T06:39:16.822957Z","iopub.status.idle":"2024-01-13T06:39:16.832415Z","shell.execute_reply.started":"2024-01-13T06:39:16.822934Z","shell.execute_reply":"2024-01-13T06:39:16.831499Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Horizontal stack to TF data","metadata":{}},{"cell_type":"code","source":"# if not OFFLINE:\n#     print(\"Stack cohesion features\")\n# #     scaler = StandardScaler()\n# #     scaler = MinMaxScaler()\n# #     test_co_scaled = scaler.fit_transform(test_co)\n# #     train_co_scaled = scaler.transform(train_co)\n    \n#     tf_train_en = hstack([tf_train, train_co[['word_diversity']]])\n#     tf_test_en = hstack([tf_test, test_co[['word_diversity']]])\n#     print(\"tf train shape\", tf_train_en.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:39:16.833606Z","iopub.execute_input":"2024-01-13T06:39:16.833868Z","iopub.status.idle":"2024-01-13T06:39:16.844412Z","shell.execute_reply.started":"2024-01-13T06:39:16.833845Z","shell.execute_reply":"2024-01-13T06:39:16.843569Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Embedding Features V3","metadata":{}},{"cell_type":"code","source":"from transformers import DebertaV2Tokenizer, DebertaV2Model\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:39:16.845569Z","iopub.execute_input":"2024-01-13T06:39:16.845888Z","iopub.status.idle":"2024-01-13T06:39:17.394489Z","shell.execute_reply.started":"2024-01-13T06:39:16.845858Z","shell.execute_reply":"2024-01-13T06:39:17.393512Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall\"\n\n# Load pre-trained model tokenizer and model\ntokenizer = DebertaV2Tokenizer.from_pretrained(model_checkpoint)\nmodel = DebertaV2Model.from_pretrained(model_checkpoint)\n\nmodel.eval()\nmodel.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:48:23.867385Z","iopub.execute_input":"2024-01-13T06:48:23.867749Z","iopub.status.idle":"2024-01-13T06:48:25.732160Z","shell.execute_reply.started":"2024-01-13T06:48:23.867723Z","shell.execute_reply":"2024-01-13T06:48:25.731220Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"DebertaV2Model(\n  (embeddings): DebertaV2Embeddings(\n    (word_embeddings): Embedding(128100, 384, padding_idx=0)\n    (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n    (dropout): StableDropout()\n  )\n  (encoder): DebertaV2Encoder(\n    (layer): ModuleList(\n      (0-11): 12 x DebertaV2Layer(\n        (attention): DebertaV2Attention(\n          (self): DisentangledSelfAttention(\n            (query_proj): Linear(in_features=384, out_features=384, bias=True)\n            (key_proj): Linear(in_features=384, out_features=384, bias=True)\n            (value_proj): Linear(in_features=384, out_features=384, bias=True)\n            (pos_dropout): StableDropout()\n            (dropout): StableDropout()\n          )\n          (output): DebertaV2SelfOutput(\n            (dense): Linear(in_features=384, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n        (intermediate): DebertaV2Intermediate(\n          (dense): Linear(in_features=384, out_features=1536, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): DebertaV2Output(\n          (dense): Linear(in_features=1536, out_features=384, bias=True)\n          (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n          (dropout): StableDropout()\n        )\n      )\n    )\n    (rel_embeddings): Embedding(512, 384)\n    (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def get_deberta_embeddings(batch_texts):\n    # Process a batch of texts and return their embeddings\n    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to('cuda')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n    return embeddings\n\n\ndef get_embeddings(data, batch_size=32):\n    embeddings = []\n    for i in tqdm(range(0, len(data['text']), batch_size), desc=\"Generating embeddings\"):\n        batch_texts = data['text'][i:i+batch_size].tolist()\n        batch_embeddings = get_deberta_embeddings(batch_texts)\n        embeddings.extend(batch_embeddings)\n\n    return np.vstack(embeddings)\n    \n\nres = get_deberta_embeddings(train['text'].iloc[0])\nprint(len(train['text'].iloc[0].split(' ')), res.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T07:06:58.966373Z","iopub.execute_input":"2024-01-13T07:06:58.967233Z","iopub.status.idle":"2024-01-13T07:06:59.004669Z","shell.execute_reply.started":"2024-01-13T07:06:58.967201Z","shell.execute_reply":"2024-01-13T07:06:59.003774Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"378 (1, 384)\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\ntrain_embeddings = get_embeddings(train, batch_size=100)\nprint(\"Train embeddings done\")\ntest_embeddings = get_embeddings(test, batch_size=100)\nprint(\"Test embeddings done\")\n\nbert_train_sparse = csr_matrix(train_embeddings)\nbert_test_sparse = csr_matrix(test_embeddings)\n\n# Concatenate deBERTa embeddings with TF-IDF features\ntf_train = hstack([tf_train, bert_train_sparse])\ntf_test = hstack([tf_test, bert_test_sparse])\n\nprint(\"Train shape\", tf_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:55:32.305927Z","iopub.execute_input":"2024-01-13T06:55:32.306852Z","iopub.status.idle":"2024-01-13T07:05:29.312486Z","shell.execute_reply.started":"2024-01-13T06:55:32.306817Z","shell.execute_reply":"2024-01-13T07:05:29.311573Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating embeddings:   0%|          | 0/449 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7c966467a4d4dacbf1f22c84ce01e5a"}},"metadata":{}},{"name":"stdout","text":"Train embeddings done\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbdaf88dfb544f9393f8e19adf5a8e53"}},"metadata":{}},{"name":"stdout","text":"Test embeddings done\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m<timed exec>:6\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sparse' is not defined"],"ename":"NameError","evalue":"name 'sparse' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# clf_nb = MultinomialNB(alpha=0.02)\n# clf_sgd = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n\n# p6 = {'n_iter': 1500, 'verbose': -1,'objective': 'binary',\n#       'metric': 'auc','learning_rate': 0.05073909898961407, \n#       'colsample_bytree': 0.726023996436955, 'colsample_bynode': 0.5803681307354022, \n#       'lambda_l1': 8.562963348932286, 'lambda_l2': 4.893256185259296, \n#       'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898}\n# clf_lgb = LGBMClassifier(**p6)\n# clf_cat = CatBoostClassifier(\n#     iterations=1000,\n#     verbose=0,\n#     l2_leaf_reg=6.6591278779517808,\n#     learning_rate=0.005689066836106983,\n#     allow_const_label=True)\n\n\n# class MetaClassifier:\n#     def __init__(self, clfs):\n#         self.clfs = clfs\n#         self.meta_clf = LogisticRegression(penalty='l2', solver='liblinear', C=0.5)\n            \n#     def fit(self, tf_train, y_train):\n#         for clf in self.clfs:\n#             clf.fit(tf_train, y_train)\n#         print(\"Individual model fitting done\")\n        \n#         meta_feats = self._stack_preds(tf_train)\n#         self.meta_clf.fit(meta_feats, y_train)\n#         print(\"Meta model fitting done\")\n    \n#     def predict_proba(self, tf_test):\n#         meta_feats = self._stack_preds(tf_test)\n#         return self.meta_clf.predict_proba(meta_feats)\n    \n#     def _stack_preds(self, data):\n#         preds = []\n#         for clf in self.clfs:\n#             pred = clf.predict_proba(data)[:, 1]\n#             preds.append(pred)\n#         return np.column_stack(preds)\n\n\n    \n# clfs = [clf_nb, clf_sgd, clf_lgb, clf_cat]\n# meta_clf = MetaClassifier(clfs)\n\n# meta_clf.fit(tf_train, y_train)\n# final_preds = meta_clf.predict_proba(tf_test)[:, 1]\n# sub['generated'] = final_preds * 0.9 + pred_cohesion * 0.1\n# sub.to_csv('submission.csv', index=False)\n# sub","metadata":{"execution":{"iopub.status.busy":"2024-01-13T06:47:51.856518Z","iopub.status.idle":"2024-01-13T06:47:51.856903Z","shell.execute_reply.started":"2024-01-13T06:47:51.856723Z","shell.execute_reply":"2024-01-13T06:47:51.856743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test.text.values) <= 5:\n    sub.to_csv('submission.csv', index=False)\nelse:\n#     clf = MultinomialNB(alpha=0.02)\n    clf = LogisticRegression(solver='liblinear', C=0.5)\n    sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n    p6 = {'n_iter': 1500,'verbose': -1,'objective': 'binary',\n          'metric': 'auc','learning_rate': 0.05073909898961407, \n          'colsample_bytree': 0.726023996436955, 'colsample_bynode': 0.5803681307354022, \n          'lambda_l1': 8.562963348932286, 'lambda_l2': 4.893256185259296, \n          'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 254, \"device\": \"gpu\"}\n    lgb = LGBMClassifier(**p6)\n    cat = CatBoostClassifier(\n        iterations=1000,\n        verbose=0,\n        l2_leaf_reg=6.6591278779517808,\n        learning_rate=0.005689066836106983,\n        allow_const_label=True,\n        task_type='GPU'\n    )\n    weights = [0.07, 0.31, 0.31, 0.31]\n \n    ensemble = VotingClassifier(estimators=[('mnb',clf),\n                                            ('sgd', sgd_model),\n                                            ('lgb',lgb), \n                                            ('cat', cat)\n                                           ],\n                                weights=weights, voting='soft', n_jobs=-1)\n    ensemble.fit(tf_train, y_train)\n    gc.collect()\n    final_preds = ensemble.predict_proba(tf_test)[:,1]\n    # update with cohesion weight\n    sub['generated'] = final_preds # * 0.98 + pred_cohesion * 0.02\n    sub.to_csv('submission.csv', index=False)\n    print(sub)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T07:13:42.876210Z","iopub.execute_input":"2024-01-13T07:13:42.877209Z","iopub.status.idle":"2024-01-13T07:15:45.622696Z","shell.execute_reply.started":"2024-01-13T07:13:42.877173Z","shell.execute_reply":"2024-01-13T07:15:45.621580Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"LGB done\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_iter` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nWarning: less than 75% gpu memory available for training. Free: 9414.125 Total: 16276.25\n","output_type":"stream"},{"name":"stdout","text":"         id  generated\n0  0000aaaa   0.029780\n1  1111bbbb   0.354078\n2  2222cccc   0.365634\n[LightGBM] [Warning] lambda_l1 is set=8.562963348932286, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.562963348932286\n[LightGBM] [Warning] lambda_l2 is set=4.893256185259296, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.893256185259296\n[LightGBM] [Warning] num_iterations is set=1500, n_iter=1500 will be ignored. Current value: num_iterations=1500\n[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}