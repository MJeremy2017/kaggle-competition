{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":6921012,"sourceType":"datasetVersion","datasetId":3972872},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7385558,"sourceType":"datasetVersion","datasetId":4292774},{"sourceId":7458263,"sourceType":"datasetVersion","datasetId":4049286},{"sourceId":4295,"sourceType":"modelInstanceVersion","modelInstanceId":3090},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899}],"dockerImageVersionId":30581,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mistral-7b + external training datasets (daigt and slimpajama)\n\n### Massive dataset alert! This notebook analyzes over 1.3 million texts, so it can be time-consuming for potentially lengthy training even on TPUs (be aware of weekly quota)\n\nThis notebook investigates the use of an pretrained LLM to identify texts generated by another LLM.\n- The `Mistral-7b-v0` and `Llama-2` were employed. `Debert-v3` models are not currently supported by TUPs because they are not partitioned.\n- Fine-tuning the LLM on TPUs reduces training time from several hours (>6 hours) on GPUs to just 43 minutes. Notebook internet access must enable to install the necessary libraries for TPU training.\n- Fine-tune a large language model (LLM) on a larger external dataset to improve its accuracy, then validate its performance using cross-validation. \n- Use Optuna to identify optimal hyperparameters, including learning rate (lr), for the target model. This Optuna study is saved as a file for future resumption.\n- Split the data from `slimpajama`, `daigt-v2-train-dataset` and `train_essay` into training and valid datasets.\n- Validate the model at the end of epochs. \n\n**[Change log]**:\n- [Version 188] Resume fine-tuning `Mistral-7b` model (batch_size=`16`, r=64, max_length=1024, epoch=1, lr=5e-5, AdamW and cosine scheduler) and `20000 ~ 100000` texts from `slimpajama` and `train_essay` with training and testing split and batch processing.\n\n- [Version 186] Fine-tune `Mistral-7b` model (batch_size=`16`, r=64, max_length=1024, epoch=1, lr=5e-5, AdamW and cosine scheduler) and 20000 texts from `slimpajama` + all texts from `daigt-v2`,  and `train_essay` with training and testing split and batch processing.\n- [Version 176] Fine-tune `Deberta-v3-large` model (batch_size=`16`, r=64, max_length=512, epoch=1, lr=5e-5, AdamW and cosine scheduler) and use three datasets (`daigt-v2`, `slimpajama` and `train_essay`) with training and testing split (testsize = 0.001) and batch processing.\n- [Version 175] Fine-tune `Mistral-7b` model (batch_size=`1024`, r=64, max_length=1024, epoch=1, lr=5e-5, AdamW and cosine scheduler) and use three datasets (`daigt-v2`, `slimpajama` and `train_essay`) with training and testing split (testsize = 0.001) and batch processing. This causes **out of memory issues**.\n- [Version 164] Fine-tune `Mistral-7b` model with 5-fold CV (GRADIENT_ACCUMULATION_STEPS=`1`, r=`64`, max_length=`1024`, epoch=`1`, lr=5e-5, AdamW and cosine scheduler) and use all the training data from daigt-v2-train-dataset\n- [Version 163] Fine-tune `Mistral-7b` model (GRADIENT_ACCUMULATION_STEPS=`2`, r=`64`, max_length=`1024`, epoch=`1`, lr=5e-5, AdamW and cosine scheduler) and use all the training data from daigt-v2-train-dataset\n- [Version 162] Fine-tune `Mistral-7b` model (GRADIENT_ACCUMULATION_STEPS=`1`, r=`64`, max_length=`1024`, epoch=`1`, lr=5e-5, AdamW and cosine scheduler) and use all the training data from daigt-v2-train-dataset\n- [Version 158] Fine-tune `Mistral-7b` model (r=`128`, max_length=`1024`, epoch=`3`, lr=5e-5, AdamW and cosine scheduler) and use all the training data from daigt-v2-train-dataset\n- [Version 157] Fine-tune `Mistral-7b` model (r=`128`, max_length=`1024`, epoch=1, lr=5e-5, AdamW and cosine scheduler) and use all the training data from daigt-v2-train-dataset\n- [Version 157] Fine-tune `Mistral-7b` model (r=64, max_length=`1024`, epoch=1, lr=5e-5, AdamW and cosine scheduler) and use all the training data from daigt-v2-train-dataset\n- [Version 156] Fine-tune `Mistral-7b` model (r=`64`, max_length=`512`, epoch=1, lr=`5e-5`, AdamW and cosine scheduler) and use all the training data from daigt-v2-train-dataset\n- [Version 146] Fine-tune `Mistral-7b` model (train number = 3000 and validation number = 600, epoch = 1). Training time = 2 hours and 14 minutes\n- [Version 125] Fine-tune `Mistral-7b` model (train number = 1,000 and validation number = 300, epoch = 1). Training time=1 hour and 15 minutes)\n- [Version 128] Fine-tune `Llama-2` model (train number = 1,000 and validation number = 300, epoch = 1). Training time=1 hour and 12 minutes.\n\n**[References]**:\nThis notebook is folked from the notebook by @YUICHI TATENO, @MARK WIJKHUIZEN and @ImperfectKitto\n-[PyTorch TPU starter - DeBERTa-v3-large (training)](https://www.kaggle.com/code/tanlikesmath/pytorch-tpu-starter-deberta-v3-large-training)\n- [[train]LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b)\n- [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train) \n- [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)","metadata":{}},{"cell_type":"markdown","source":"# Install library","metadata":{}},{"cell_type":"code","source":"# Install package for inferences\n!pip install -qq --no-deps /kaggle/input/daigt-pip/peft-0.6.0-py3-none-any.whl\n!pip install -qq --no-deps /kaggle/input/daigt-pip/transformers-4.35.0-py3-none-any.whl\n!pip install -qq --no-deps /kaggle/input/daigt-pip/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -qq --no-deps /kaggle/input/daigt-pip/optimum-1.14.0-py3-none-any.whl","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-01-21T22:17:00.983518Z","iopub.execute_input":"2024-01-21T22:17:00.984337Z","iopub.status.idle":"2024-01-21T22:17:03.923643Z","shell.execute_reply.started":"2024-01-21T22:17:00.984305Z","shell.execute_reply":"2024-01-21T22:17:03.922562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install packaages for training on TPUs (notebook internet must enable)\n!pip install datasets\n!pip install -qq optuna\n!pip install -qq sentencepiece==0.1.99 \n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-01-21T22:17:03.925553Z","iopub.execute_input":"2024-01-21T22:17:03.92583Z","iopub.status.idle":"2024-01-21T22:17:21.97873Z","shell.execute_reply.started":"2024-01-21T22:17:03.925803Z","shell.execute_reply":"2024-01-21T22:17:21.977584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Imports","metadata":{}},{"cell_type":"code","source":"import torch, transformers, sklearn, os, gc, re, random, time, sys, optuna\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom accelerate import cpu_offload, dispatch_model\nfrom accelerate.utils.modeling import infer_auto_device_map\nfrom tqdm import tqdm\nfrom tqdm.auto import tqdm\nfrom numpy import save\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\ntqdm.pandas()\n\npd.options.display.max_rows = 999\npd.options.display.max_colwidth = 99\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-01-21T22:17:21.980225Z","iopub.execute_input":"2024-01-21T22:17:21.980542Z","iopub.status.idle":"2024-01-21T22:17:21.98836Z","shell.execute_reply.started":"2024-01-21T22:17:21.98051Z","shell.execute_reply":"2024-01-21T22:17:21.987724Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports for training on TPUs","metadata":{}},{"cell_type":"code","source":"## Imports for Transformers and PEFT (Parameter-Efficient Fine-Tuning)\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom transformers import (\n    LlamaModel, LlamaConfig, LlamaForSequenceClassification, BitsAndBytesConfig,\n    AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModelForSequenceClassification,\n    DataCollatorWithPadding, MistralForSequenceClassification\n) \n\n## Imports for TPU XLA \nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\nxr.use_spmd() # To enable PyTorch/XLA SPMD execution mode for automatic parallelization\nassert xr.is_spmd() == True \n\n# \"experimental\" XLA packages\nimport torch_xla.experimental.xla_sharding as xs \nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:17:21.989342Z","iopub.execute_input":"2024-01-21T22:17:21.989612Z","iopub.status.idle":"2024-01-21T22:17:22.00415Z","shell.execute_reply.started":"2024-01-21T22:17:21.989583Z","shell.execute_reply":"2024-01-21T22:17:22.003532Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common functions","metadata":{}},{"cell_type":"code","source":"# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# Seed the same seed to all \ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\nSEED = 42\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:17:22.006226Z","iopub.execute_input":"2024-01-21T22:17:22.00649Z","iopub.status.idle":"2024-01-21T22:17:22.015542Z","shell.execute_reply.started":"2024-01-21T22:17:22.006463Z","shell.execute_reply":"2024-01-21T22:17:22.0149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load training data\n\nTraining data include 1,378 sample tests from the competition, 20,450 texts from [DAIGT V2 Train Dataset](https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset), and 1,324,128 texts from [slimpajama dataset](https://www.kaggle.com/datasets/chg0901/slimpajama-train-chunk1-sel)\n\nTotal number of texts = 1,345,956","metadata":{}},{"cell_type":"code","source":"DEBUG = False # True: 2 datasets and False: 3 datasets \n# Cross validation\ndef cv_split(train_data):\n    N_FOLD = 5 # Number of folders\n    skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n    X = train_data.loc[:, train_data.columns != \"label\"]\n    y = train_data.loc[:, train_data.columns == \"label\"]\n    # Split the train into 5 folds\n    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n        train_data.loc[valid_index, \"fold\"] = fold\n\n    print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n    return train_data\n\ndef load_train_data():\n    train_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\", sep=',')\n    train_prompts_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\", sep=',')\n\n    # rename column generated to label and remove used 'id' and 'prompt_id' columns\n    # Label: 1 indicates generated texts (by LLMs) \n    train_df = train_df.rename(columns={'generated': 'label'})\n    train_df = train_df.reset_index(drop=True)\n    train_df = train_df[[\"text\", \"label\"]]\n    print(f\"Total number of training essay texts {len(train_df)}\")\n   \n    \n    if DEBUG:\n         # Include external daigt_df data\n        daigt_df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n        # Select the texts for the 7 prompts that likely fit the competition test set\n        daigt_df = daigt_df[daigt_df['RDizzl3_seven']]\n        daigt_df = daigt_df.rename(columns={'generated': 'label'})\n        # We only need 'text' and 'label' columns\n        daigt_df = daigt_df[[\"text\", \"label\"]]\n        # dropping ALL duplicate values \n        daigt_df.drop_duplicates(subset=\"text\", keep=False, inplace=True) \n        print(f\"Total number of external DAIGT sample texts = {len(daigt_df)}\")\n        train_data = pd.concat([train_df, daigt_df])\n    else:\n        START = 20000\n        END = 100000\n        # Include slimpajama-training datast\n        slimpajama_df = pd.read_csv(\"/kaggle/input/slimpajama-train-chunk1-sel/merged_df_sel150_600_1324132.csv\", sep=',')\n        # Sample the dataset\n        slimpajama_df = slimpajama_df[slimpajama_df['meta'] == 'RedPajamaC4']\n        slimpajama_df = slimpajama_df[START:END]\n        slimpajama_df = slimpajama_df[[\"text\", \"label\"]]\n        # dropping ALL duplicate values \n        slimpajama_df.drop_duplicates(subset=\"text\", keep=False, inplace=True)\n        #display(slimpajama_df)\n        print(f\"Total number of slimpajama training texts = {len(slimpajama_df)}\")\n        train_data = pd.concat([train_df, slimpajama_df]) \n        \n    train_data.reset_index(inplace=True, drop=True)    \n    # print(f\"Train data has shape: {train_data.shape}\")\n    print(f\"Train data {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:17:22.016602Z","iopub.execute_input":"2024-01-21T22:17:22.016852Z","iopub.status.idle":"2024-01-21T22:17:22.027838Z","shell.execute_reply.started":"2024-01-21T22:17:22.016828Z","shell.execute_reply":"2024-01-21T22:17:22.027238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = load_train_data()\n# Train dataset is too large to do 5 fold CV\n# train_data = cv_split(train_data)\ndisplay(train_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:17:22.028905Z","iopub.execute_input":"2024-01-21T22:17:22.029299Z","iopub.status.idle":"2024-01-21T22:17:23.28547Z","shell.execute_reply.started":"2024-01-21T22:17:22.029246Z","shell.execute_reply":"2024-01-21T22:17:23.284857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the LLM model on TPUs\n\nNLTK package is used to correct ","metadata":{}},{"cell_type":"code","source":"def pre_processing_text(text):\n    corrected_text = text.replace('\\n', ' ')\n    #corrected_text = TextBlob(corrected_text).correct()\n    return corrected_text","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:17:23.286444Z","iopub.execute_input":"2024-01-21T22:17:23.286721Z","iopub.status.idle":"2024-01-21T22:17:23.290312Z","shell.execute_reply.started":"2024-01-21T22:17:23.286693Z","shell.execute_reply":"2024-01-21T22:17:23.289596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"import transformers\n\nclass TrainModelTPU():\n    def __init__(self, model, train_data, **params):\n        self.train_data = train_data\n        self.LR = params['lr'] # Learning rate\n        self.R = params['r'] # 'r' value for Lora layer\n        self.NUM_EPOCHS = params['num_epochs'] # Training Epoch\n        # Fixed parameters\n        self.NUM_LABELS = 1 # Total Number of Labels (0:human texts, 1:LLM generated texts)\n        self.MAX_LENGTH = params['max_length']\n        self.BATCH_SIZE = params['batch_size']\n        self.DEVICE = xm.xla_device() # Initialize TPU Device\n        self.NUM_WARMUP_STEPS = 0 # Number of Warmup Steps\n        self.GRADIENT_ACCUMULATION_STEPS = 1\n        # The model\n        self.MODEL = model\n        \n    # Load pretrained LLM and tokenizer\n    def load_model(self):\n        if \"mistral_7b\" == self.MODEL:\n            MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"  # Mistral\n        if \"llama-2_7b\" == self.MODEL:    \n            MDEL_PATH = \"/kaggle/input/llama-2/pytorch/7b-hf/1\"  # llama\n        # Load the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        # `bfloat16` is suitable for deep learning for better convergences during training\n        base_model = LlamaForSequenceClassification.from_pretrained(MODEL_PATH,\n                                                                num_labels=self.NUM_LABELS,\n                                                                torch_dtype=torch.bfloat16)\n        # No idea why this is needed\n        base_model.config.pretraining_tp = 1 # 1 is 7b\n        # Assign Padding TOKEN\n        base_model.config.pad_token_id = self.tokenizer.pad_token_id\n        # print(base_model)        \n        # LoRa\n        peft_config = LoraConfig(\n            r=self.R,  # Use larger 'r' value increase more parameters during training\n            lora_dropout=0.1,\n            bias='none',\n            inference_mode=False,\n            task_type=TaskType.SEQ_CLS,\n            # Only Use Output and Values Projection\n            target_modules=['o_proj', 'v_proj'], # layer names for llama 2 model\n        )\n        # Continue training on previous epoch\n        # Load the new PEFT model\n#         self.model = get_peft_model(base_model, peft_config)\n        peft_path = '/kaggle/input/mistral-7b-v0-for-llm-detecting-competition/mistral_7b/mistral_7b_TPU'\n        self.model = PeftModel.from_pretrained(base_model, peft_path, is_trainable=True)\n        # Display Trainable Parameters to make sure we load the model successfully\n        self.model.print_trainable_parameters()\n        # self.model = self.model.merge_and_unload()\n        print(\"Complete loading pretrained LLM model\")\n    \n    # Save the trained model as output files\n    def save_model(self):\n        self.model = self.model.cpu()# Move model first on CPU before saving weights\n        # Model saving path\n        SAVE_PATH = f'/kaggle/working/{self.MODEL}/{self.MODEL}_TPU/'\n        # Save the entire fine-tuned model\n        self.model.save_pretrained(SAVE_PATH, save_adapter=True, save_config=True) \n        # Save tokenizer for inference\n        self.tokenizer.save_pretrained(SAVE_PATH)\n        # Only saving the newly trained weights\n        torch.save(dict([(k,v) for k, v in self.model.named_parameters() if v.requires_grad]), \n                   SAVE_PATH + 'model_weights.pth')\n        print(f\"Save the model and tokenizers to {SAVE_PATH}\")\n    \n    # Disply trainable layers of LLM\n    def display_model_layers(self):        \n        # Dispaly trainable layers for verification\n        trainable_layers = []\n        n_trainable_params = 0\n        for name, param in self.model.named_parameters():\n            # Layer Parameter Count\n            n_params = int(torch.prod(torch.tensor(param.shape)))\n            # Only Trainable Layers\n            if param.requires_grad:\n                # Add Layer Information\n                trainable_layers.append({\n                    '#param': n_params,\n                    'name': name,\n                    'dtype': param.data.dtype,\n                    'params': param\n                })\n                n_trainable_params += n_params\n\n        display(pd.DataFrame(trainable_layers))\n        print(f\"Number of trainable parameters: {n_trainable_params:,} \"\n              f\"Number of trainable layers: {len(trainable_layers)}\")\n    \n    def create_optimizer_scheduler(self, STEPS_PER_EPOCH):        \n        # Optimizer (Adam)\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.LR, weight_decay=0.01)\n        # Cosine Learning Rate With Warmup\n        lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n                                    optimizer=optimizer,\n                                    num_warmup_steps=self.NUM_WARMUP_STEPS,\n                                    num_training_steps=STEPS_PER_EPOCH * self.NUM_EPOCHS)\n        # Set the data type for the optimizer's state (e.g., momentum buffers)\n        for state in optimizer.state.values():\n            for k, v in state.items():\n                if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n                    state[v] = v.to(dtype=torch.float32)\n        print(\"Complete creating optimizer and lr scheduler\")\n        print(\"optimizer\", optimizer)\n        print(\"lr_scheduler\", lr_scheduler)\n        return optimizer, lr_scheduler\n        \n    def partition_mesh(self):\n        # Number of TPU Nodes to ensure we can access TPUs and partition the model into mesh\n        num_devices = xr.global_runtime_device_count()\n        mesh_shape = (1, num_devices, 1)\n        print(f'Number_DEVICES: {num_devices}')\n        device_ids = np.array(range(num_devices))\n        mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n        partition_module(self.model, mesh)\n        return mesh        \n        \n     # Create a training dataset \n    def create_dataset(self, N_SAMPLES, INPUT_IDS, ATTENTION_MASKS, GENERATED, mesh):\n        IDXS = np.arange(N_SAMPLES-(N_SAMPLES%self.BATCH_SIZE))\n        while True:\n            # Shuffle Indices\n            # np.random.shuffle(IDXS)\n            # Iterate Over All Indices Once\n            for idxs in IDXS.reshape(-1, self.BATCH_SIZE):\n                input_ids = torch.tensor(INPUT_IDS[idxs]).to(self.DEVICE)\n                attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(self.DEVICE)\n                labels = torch.tensor(GENERATED[idxs]).to(self.DEVICE)\n                # Shard Over TPU Nodes\n                xs.mark_sharding(input_ids, mesh, (0, 1))\n                xs.mark_sharding(attention_mask, mesh, (0, 1))\n                xs.mark_sharding(labels, mesh, (0, 1))\n                yield input_ids, attention_mask, labels\n    \n    # Define function to encode text data in batches\n    #encodes a batch of texts and returns the texts' ids and attention masks\n    def batch_encode(self, texts):\n        input_ids = []\n        attention_mask = []\n\n        for i in tqdm(range(0, len(texts), self.BATCH_SIZE)):\n            batch_texts = texts[i:i+self.BATCH_SIZE]\n            inputs = self.tokenizer.batch_encode_plus(batch_texts,\n                                                      padding='max_length', # Pad texts to maximum length\n                                                      max_length=self.MAX_LENGTH, # Maximum token length\n                                                      truncation=True, # Truncate texts if they are too long\n                                                      return_tensors='np', # Return Numpy array\n                                                      return_attention_mask=True,\n                                                      return_token_type_ids=False\n                                                     )\n            input_ids.extend(inputs['input_ids'])\n            attention_mask.extend(inputs['attention_mask'])\n        tokens = {'input_ids': np.asarray(input_ids), 'attention_mask': np.asarray(attention_mask)}\n        # save to npy file (Too large to save)\n        # save('train_data_input_ids.npy', tokens['input_ids'])\n        # save('train_data_attention_mask.npy', tokens['attention_mask'])\n        return tokens\n    \n    # Validate the model\n    def valid_model(self, valid_df):\n        # Compute total samples and number of steps in one epochs\n        N_SAMPLES = len(valid_df)       \n        print(f\"Start validating the model with number of sample {N_SAMPLES}\")\n        # Tokenize Texts\n        tokens = self.tokenizer(valid_df['text'].tolist(), \n                                padding='max_length', # Pad texts to maximum length\n                                max_length=self.MAX_LENGTH, # Maximum token length\n                                truncation=True, # Truncate texts if they are too long\n                                return_tensors='np', # Return Numpy array\n                                )\n        # Input IDs are the token IDs\n        INPUT_IDS = tokens['input_ids']\n        # Attention Masks to Ignore Padding Tokens\n        ATTENTION_MASKS = tokens['attention_mask']\n        # Generated By AI Label of Texts\n        GENERATED = valid_df['label'].values.reshape(-1,1).astype(np.float32)\n        # Create a valid dataset \n        VALID_DATASET = self.create_dataset(N_SAMPLES, INPUT_IDS, ATTENTION_MASKS, GENERATED, self.mesh)\n                \n        # Compute the number of batches \n        IDXS = np.array_split(np.arange(N_SAMPLES), max(1, N_SAMPLES // self.BATCH_SIZE))\n        LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n        METRICS = {'loss': [], \n                   'auc': {'y_true': [], 'y_pred': []} }\n        STEPS = N_SAMPLES // self.BATCH_SIZE\n        for step in range(STEPS):\n            # Enable inference mode using `no_grad`\n            with torch.no_grad():\n                # Get Batch\n                input_ids, attention_mask, labels = next(VALID_DATASET)\n                 # Forward Pass\n                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                # Logits Float32\n                logits = outputs.logits.to(dtype=torch.float32)\n                # Backward Pass\n                loss = LOSS_FN(logits, labels)\n                # Update Metrics And Progress Bar\n                METRICS['loss'].append(float(loss))\n                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n                # print(f\"Complete updating metrics for Step {step} in {time.time() - start: .1f} seconds\")\n        loss = np.mean(METRICS['loss'])\n        roc_auc = sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred'])\n        # Compute and display the validation results\n        print(f\"Number of validation data {len(valid_df)}\\n\"\n              f\"µ_loss: {loss: .3f}\\n\"\n              f\"µ_auc: {roc_auc:.3f}\")\n        return {\"eval_loss\": loss, \"eval_roc_auc\": roc_auc}\n \n    # Train the model \n    def train_model(self):\n        self.mesh = self.partition_mesh()\n        print(f\"Total number of train data = {len(self.train_data)}\")\n        # Preprocess the text\n        train_data['text'] = train_data['text'].map(lambda text: pre_processing_text(text))\n        # Split the training dataset into training and test data\n        X = train_data['text'].tolist()\n        y = train_data['label'].tolist()\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=600, random_state=SEED)\n        train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n        valid_df = pd.DataFrame({'text': X_test, 'label': y_test})\n        print(f\"Training dataset size = {len(train_df)}\")\n        print(f\"Validate dataset Size = {len(valid_df)}\")\n    \n        # Compute total samples and number of steps in one epochs\n        N_SAMPLES = len(train_df)\n        # Compute the total steps per epochs\n        STEPS_PER_EPOCH = N_SAMPLES // self.BATCH_SIZE\n        print(f'BATCH_SIZE: {self.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')\n               \n        # Tokenize Texts\n#         tokens = self.tokenizer(train_df['text'].tolist(), \n#                                 padding='max_length', # Pad texts to maximum length\n#                                 max_length=self.MAX_LENGTH, # Maximum token length\n#                                 truncation=True, # Truncate texts if they are too long\n#                                 return_tensors='np', # Return Numpy array\n#                                 )\n        tokens = self.batch_encode(train_df['text'].tolist())\n        print(\"Complete to tokenize inputs\")\n        # Input IDs are the token IDs\n        INPUT_IDS = tokens['input_ids']\n        # Attention Masks to Ignore Padding Tokens\n        ATTENTION_MASKS = tokens['attention_mask']\n        # Generated By AI Label of Texts\n        GENERATED = train_df['label'].values.reshape(-1,1).astype(np.float32)\n        print(f'INPUT_IDS shape: {INPUT_IDS.shape}\\n'\n              f'ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}\\n'\n              f'GENERATED shape: {GENERATED.shape}')\n        \n        # Create a train dataset\n        TRAIN_DATASET = self.create_dataset(N_SAMPLES, INPUT_IDS, ATTENTION_MASKS, GENERATED, self.mesh)     \n        print(\"Complete creating the datasets\")\n        # Create optimizer and lr_scheduler\n        optimizer, lr_scheduler = self.create_optimizer_scheduler(STEPS_PER_EPOCH)\n        \n        # Put Model In Train Modus\n        self.model.train()\n        # Loss Function, basic Binary Cross Entropy\n        # LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n        LOSS_FN = torch.nn.MSELoss().to(dtype=torch.float32)\n        eval_scores = []\n        # Training loop goes through each epoch\n        for epoch in tqdm(range(self.NUM_EPOCHS)):\n            METRICS = {'loss': [], \n                       'auc': {'y_true': [], 'y_pred': []} }\n            # Go through each step\n            for step in range(STEPS_PER_EPOCH):\n                print(f'=== Start Step {step} === ')\n                # Zero Out Gradients\n                optimizer.zero_grad()\n                # Get Batch\n                input_ids, attention_mask, labels = next(TRAIN_DATASET)\n                # Test the TRAIN_DATASET for debugging first record\n                # Forward Pass\n                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                # Logits Float32\n                logits = outputs.logits.to(dtype=torch.float32)\n                # Backward Pass\n                loss = LOSS_FN(logits, labels)\n                # backward propagation pass\n                loss.backward()\n                # Update Weights\n                optimizer.step()\n                xm.mark_step()\n                # Update Learning Rate Scheduler\n                lr_scheduler.step()\n                # Update Metrics And Progress Bar\n                METRICS['loss'].append(float(loss))\n                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n                # print(f\"Complete updating metrics {METRICS}\")\n                if np.unique(METRICS['auc']['y_true']).size == 2:\n                    metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n                    metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n                    metrics += ', µ_auc: {:.3f}'.format(\n                        sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred'])\n                    )\n\n                    lr = optimizer.param_groups[0]['lr']\n                    print(f'{epoch:02}/{self.NUM_EPOCHS:02} | {step:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}')\n            avg_loss = np.mean(METRICS['loss'])\n            roc_auc_score = sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'],\n                                                          METRICS['auc']['y_pred'])\n            print(f'\\n=== Finish Training on Epoch {epoch} ===\\n'\n                  f'Average Loss = {avg_loss: .5f}, ROC Accuracy Score {roc_auc_score:.5f}')\n            # Validate the model at the end of epochs\n            result = self.valid_model(valid_df) \n            eval_roc_auc = float(result['eval_roc_auc'])\n            eval_scores.append(eval_roc_auc)\n        avg_score = np.mean(eval_scores)\n        print(f'\\n=== Finish Training  ==='\n              f'Validate Average ROC Score = {avg_score:.5f}')\n        return avg_score\n\n    # Clear the memory\n    def clear_memory(self):\n        del self.model, self.tokenizer\n        libc.malloc_trim(0)\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:17:23.29163Z","iopub.execute_input":"2024-01-21T22:17:23.291987Z","iopub.status.idle":"2024-01-21T22:17:23.332545Z","shell.execute_reply.started":"2024-01-21T22:17:23.291958Z","shell.execute_reply":"2024-01-21T22:17:23.331862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PyTorch XLA-specific imports\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.debug.metrics as met\n\nfrom torch.utils.data import DataLoader, Dataset\n# A class converts dataframe to Pytorch Dataset\nclass DebertaDataset(Dataset):    \n    def __init__(self, tokenizer, max_length, data):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.MAX_LENGTH = max_length\n        self.data = data\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, index):\n        # get data by index\n        row = self.data.iloc[index]\n\n        # clean and tokenize\n        text = pre_processing_text(row['text'])\n        inputs = self.tokenizer(text, truncation=True,\n                                max_length=self.MAX_LENGTH,\n                                padding=\"max_length\",\n                                )\n        # Create ids, mask and target tensors\n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        label = torch.tensor(row['label'], dtype=torch.float32)\n    \n        # return ids, mask, target for batch\n        return {\"input_ids\" : ids,\n                \"attention_mask\" : mask,\n                \"label\" : label}\n\nclass DebertaModelTrainer(TrainModelTPU): \n    def __init__(self, model, train_data, **params):\n        TrainModelTPU.__init__(self, model, train_data, **params)\n        self.NUM_WORKERS = 8 \n        self.NUM_LABELS = 1\n        self.NUM_WARMUP_STEPS = 0 # Number of Warmup Steps\n        self.STEPS = 50 # Display the progress each 50 steps\n        self.DEVICE = xm.xla_device() # Get TPU device \n        \n    # Load pretrained  LLM and tokenizer\n    def load_model(self):\n        start = time.time()\n        MODEL_PATH = f\"/kaggle/input/huggingfacedebertav3variants/{self.MODEL}\"\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        # `bfloat16` is suitable for deep learning for better convergences during training\n        base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH,\n                                                                        num_labels=self.NUM_LABELS,\n                                                                        torch_dtype=torch.bfloat16)\n        # No idea why this is needed\n        base_model.config.pretraining_tp = 1 # 1 is 7b\n        # Assign Padding TOKEN\n        base_model.config.pad_token_id = self.tokenizer.pad_token_id\n        # print(base_model)\n        \n        # LoRa\n        peft_config = LoraConfig(\n            r=self.R,  # Use larger 'r' value increase more parameters during training\n            lora_dropout=0.1, # reduce overfitting\n            bias='none',\n            inference_mode=False,\n            task_type=TaskType.SEQ_CLS,\n            # Only Use Output and Values Projection\n            target_modules=['query_proj', 'value_proj'],\n        )\n        # Continue training on previous epoch\n        # Load the new PEFT model\n        self.model = get_peft_model(base_model, peft_config)\n        self.model = self.model.merge_and_unload()    \n        print(f\"Complete loading pretrained LLM model {time.time() - start:.1f} seconds\")\n    \n    def create_data_loader(self, ds):\n        try:\n            # defining data samplers and loaders \n            data_sampler = torch.utils.data.distributed.DistributedSampler(\n                                    ds,\n                                    num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n                                    rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n                                    shuffle=True)\n            data_loader = torch.utils.data.DataLoader(ds,\n                                                      batch_size=self.BATCH_SIZE,\n                                                      sampler=data_sampler,\n                                                      drop_last=True,\n                                                      num_workers=self.NUM_WORKERS)\n            return data_loader\n        except Exception as err:\n            print(f\"Something went wrong when loading the data\")\n            print(f\"Unexpected Error {err}, {type(err)}\")\n            sys.exit(-1) \n    \n    def load_dataset(self):\n        print(f\"Total number of train data = {len(self.train_data)}\")\n        train_data = self.train_data\n        # Preprocess the text\n        train_data['text'] = train_data['text'].map(lambda text: pre_processing_text(text))\n        \n        # Split the training dataset into training and test data\n        X = train_data['text'].tolist()\n        y = train_data['label'].tolist()\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=600, random_state=SEED)\n        train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n        valid_df = pd.DataFrame({'text': X_test, 'label': y_test})\n        # create the dataset and pass tokenizer as function argum\n        train_ds = DebertaDataset(self.tokenizer, self.MAX_LENGTH, train_df)\n        valid_ds = DebertaDataset(self.tokenizer, self.MAX_LENGTH, valid_df)\n        # print(train_tokenized_ds)\n        return train_ds, valid_ds\n    \n    # Evaluate the model with valid dataset and return the loss on testing (valid) dataset\n    def eval_loop(self, valid_loader, loss_fn):\n        start = time.time()\n        self.model.eval() # put model in eval mode \n        eval_targets = []\n        eval_outputs = []\n        for bi, input_data in enumerate(valid_loader): # enumerate through dataloader\n            # put valid data tensors onto TPU device\n            for k,v in input_data.items():\n                input_data[k] = v.to(self.DEVICE)\n\n            # pass ids to model\n            with torch.no_grad(): \n                outputs = self.model(input_ids=input_data['input_ids'],\n                                     attention_mask=input_data['attention_mask'])\n\n            # Add the outputs and targets to a list \n            targets = input_data['label'].cpu().detach().tolist()\n            outputs = outputs['logits'].cpu().detach().tolist()\n            eval_targets.extend(targets)\n            eval_outputs.extend(outputs)    \n            del targets, outputs\n            gc.collect() # delete for memory conservation\n\n        # calculate loss\n        loss = loss_fn(torch.tensor(eval_outputs), torch.tensor(eval_targets))\n        # since the loss is on all 8 cores, reduce the loss values and print the average\n        loss_reduced = xm.mesh_reduce('loss_reduce',loss, lambda x: sum(x) / len(x)) \n        # print valid loss\n        xm.master_print(f'Complete evaluation. Valid loss={loss_reduced} in {time.time() - start:.2f} seconds.')\n        return loss_reduced\n        \n    def train_loop(self, train_loader, optimizer, lr_scheduler, loss_fn):\n        self.model.train() # put model in training mode\n        # A loop for training the model\n        for step, input_data in enumerate(train_loader): # enumerate through the dataloader\n            start = time.time()\n            # Move all training data tensors onto TPU device\n            for k,v in input_data.items():\n                input_data[k] = v.to(self.DEVICE)\n            # Set gradient to 0\n            optimizer.zero_grad()\n            # Pass input data\n            outputs = self.model(input_ids=input_data['input_ids'],\n                                 attention_mask=input_data['attention_mask'])\n            logits = outputs.logits.squeeze()\n            labels = input_data['label']\n            # Compute the loss\n            loss = loss_fn(logits, labels)\n            # Reduce (aggregate) all the losses on 8 cores\n            if step % self.STEPS == 0:\n                # since the loss is on all 8 cores, reduce the loss values and print the average\n                loss_reduced = xm.mesh_reduce('loss_reduce', loss, lambda x: sum(x) / len(x)) \n                # print will only print once (not from all 8 cores)\n                xm.master_print(f'step={step}, loss={loss_reduced}')\n            # backpropagate\n            loss.backward()\n            # Use PyTorch XLA optimizer stepping\n            xm.optimizer_step(optimizer)\n            lr_scheduler.step()\n            xm.master_print(f\"=== Finish training loop step = {step} in {time.time() - start:.2f} seconds.\")\n         \n    # Train the model \n    def train_model(self):\n        train_dataset, valid_dataset = self.load_dataset()\n        # Create Data loader \n        train_loader = self.create_data_loader(train_dataset)\n        valid_loader = self.create_data_loader(valid_dataset)\n        # PyTorch XLA-specific dataloading\n        train_loader = pl.MpDeviceLoader(train_loader, self.DEVICE) \n        valid_loader = pl.MpDeviceLoader(valid_loader, self.DEVICE)\n        # use the xmp.MpModelWrapper from PyTorch XLA to save memory when initializing the model\n        mx = xmp.MpModelWrapper(self.model)\n        self.model = mx.to(self.DEVICE) # put model onto the current TPU core\n        print('Complete loading model and dataloader')        \n        N_SAMPLES = len(train_dataset)\n        STEPS_PER_EPOCH = N_SAMPLES // self.BATCH_SIZE\n        NUM_TRAINING_STEPS = STEPS_PER_EPOCH * self.NUM_EPOCHS\n        print(f'BATCH_SIZE: {self.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, '\n              f'STEPS_PER_EPOCH: {STEPS_PER_EPOCH}, NUM_TRAINING_STEPS: {NUM_TRAINING_STEPS}')        \n        # ----------------------------------------------------------------\n        # Optimizer (AdamW)\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.LR, weight_decay=0.01)\n        # Cosine Learning Rate With Warmup\n        lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n                                        optimizer=optimizer,\n                                        num_warmup_steps=self.NUM_WARMUP_STEPS,\n                                        num_training_steps=NUM_TRAINING_STEPS)\n        # Loss function using basic Binary Cross Entropy\n        loss_fn = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n        eval_scores = []\n        # Train the model on epochs\n        for epoch in range(self.NUM_EPOCHS):\n            gc.collect() # I use a lot of gc.collect() statement to hopefully prevent OOM problems\n            # Train the model\n            self.train_loop(train_loader, optimizer, lr_scheduler, loss_fn)\n            gc.collect()\n            # call evaluation loop:\n            eval_score = self.eval_loop(valid_loader, loss_fn)\n            gc.collect()\n            eval_scores.append(eval_score)\n        avg_score = np.mean(eval_scores)\n        print(f'\\n=== Finish Training  ==='\n              f'Validate Average ROC Score = {avg_score:.5f}')\n        return avg_score       ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-21T22:17:23.33371Z","iopub.execute_input":"2024-01-21T22:17:23.333982Z","iopub.status.idle":"2024-01-21T22:17:23.361956Z","shell.execute_reply.started":"2024-01-21T22:17:23.333954Z","shell.execute_reply":"2024-01-21T22:17:23.361325Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Optuna to find the optimal hyper-parameters\nTrain and Save the model with best parameters ","metadata":{}},{"cell_type":"code","source":"display(train_data.head(3))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-21T22:17:23.362937Z","iopub.execute_input":"2024-01-21T22:17:23.363423Z","iopub.status.idle":"2024-01-21T22:17:23.376605Z","shell.execute_reply.started":"2024-01-21T22:17:23.36339Z","shell.execute_reply":"2024-01-21T22:17:23.375927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start optuna study to hyper-parameter tuning\nbest_score = -1.0\n# Find the optimal learning rate\ndef objective(trial, model_name, train_data):\n    # Parameters\n    params = {\n        'lr': trial.suggest_float('learning_rate', 1e-7, 1e-3, log=True),\n        'r': 64, # Default: 64\n        'num_epochs': 1,\n    }\n    # Create a trainer \n    trainer = TrainModelTPU(model_name, train_data, **params) \n    eval_score = trainer.train_model()\n    # Save the model is the avg score > current best score\n    global best_score\n    if eval_score > best_score:\n        best_score = eval_score\n        # Save all the fold models\n        trainer.save_model()\n    # Clean up\n    trainer.clear_memory()\n    del trainer\n    return eval_score  # Maximal the average 'roc_auc' metric\n\ndef train_model_with_optuna(model_name, train_data):\n    # # Create a study to find the optimal hyper-parameters\\\n    study_name = f\"{model}_study\"\n    study_file = f\"/kaggle/working/{study_name}.db\"\n    # Delete the study file if exits\n    if os.path.isfile(study_file):\n        os.remove(f'{study_file}') \n\n    study = optuna.create_study(direction=\"maximize\", study_name=study_name,\n                                storage=\"sqlite:///\" + f\"{study_file}\", # Storage path of the database keeping the study results\n                                load_if_exists=False) # True: Resume the study, False: Createa new one\n    # Set up the timeout to avoid runing out of quote\n    study.optimize(lambda trial: objective(trial, model_name, train_data), \n                   timeout=600, n_jobs=1, n_trials=10,\n                   show_progress_bar=True, gc_after_trial=True)\n    print(f\"Best parameters: {study.best_params}\")\n    params = study.best_params # Obtain the optimal parameters","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-21T22:17:23.377516Z","iopub.execute_input":"2024-01-21T22:17:23.377756Z","iopub.status.idle":"2024-01-21T22:17:23.385637Z","shell.execute_reply.started":"2024-01-21T22:17:23.37773Z","shell.execute_reply":"2024-01-21T22:17:23.385061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model with best parameters","metadata":{}},{"cell_type":"code","source":"def train_model(model_name, train_data):\n    # Parameters\n    params = {\n        'lr': 5e-5, # learning rate\n        'r': 64, # Lora's r value (Default: 64)\n        'num_epochs': 1, # number of epochs \n        'batch_size': 128,\n        'max_length': 1024\n    }\n    if model_name == 'mistral_7b':\n        params['batch_size'] = 16\n        # Create a trainer \n        trainer = TrainModelTPU(model_name, train_data, **params)\n    elif 'deberta' in model_name:\n        params['max_length'] = 512\n        trainer = DebertaModelTrainer(model_name, train_data, **params)\n    trainer.load_model()# Load the pretrained LLM and tokenizer \n    eval_score = trainer.train_model()\n    # Save the model is the avg score > current best score\n    # Save all the fold models\n    trainer.save_model()\n    # Clean up\n    trainer.clear_memory()\n    del trainer\n    print(f\"=== Finish training the model {model_name} with score = {eval_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:17:23.386567Z","iopub.execute_input":"2024-01-21T22:17:23.386851Z","iopub.status.idle":"2024-01-21T22:17:23.396831Z","shell.execute_reply.started":"2024-01-21T22:17:23.386806Z","shell.execute_reply":"2024-01-21T22:17:23.396285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"mistral_7b\" # \"mistral_7b\" # \"deberta-v3-small\"  \ntrain_model(model_name, train_data)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-01-21T22:17:23.399029Z","iopub.execute_input":"2024-01-21T22:17:23.39939Z"},"trusted":true},"execution_count":null,"outputs":[]}]}